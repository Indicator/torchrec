


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Index &mdash; TorchRec 0.9.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="#" />
    <link rel="search" title="Search" href="search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  0.9.0.dev20240708+cpu
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchrec.datasets.html">torchrec.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.datasets.scripts.html">torchrec.datasets.scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.distributed.html">torchrec.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.distributed.planner.html">torchrec.distributed.planner</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.distributed.sharding.html">torchrec.distributed.sharding</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.fx.html">torchrec.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.inference.html">torchrec.inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.models.html">torchrec.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.modules.html">torchrec.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.optim.html">torchrec.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.quant.html">torchrec.quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.sparse.html">torchrec.sparse</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Index</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              

<h1 id="index">Index</h1>

<div class="genindex-jumpbox">
 <a href="#A"><strong>A</strong></a>
 | <a href="#B"><strong>B</strong></a>
 | <a href="#C"><strong>C</strong></a>
 | <a href="#D"><strong>D</strong></a>
 | <a href="#E"><strong>E</strong></a>
 | <a href="#F"><strong>F</strong></a>
 | <a href="#G"><strong>G</strong></a>
 | <a href="#H"><strong>H</strong></a>
 | <a href="#I"><strong>I</strong></a>
 | <a href="#J"><strong>J</strong></a>
 | <a href="#K"><strong>K</strong></a>
 | <a href="#L"><strong>L</strong></a>
 | <a href="#M"><strong>M</strong></a>
 | <a href="#N"><strong>N</strong></a>
 | <a href="#O"><strong>O</strong></a>
 | <a href="#P"><strong>P</strong></a>
 | <a href="#Q"><strong>Q</strong></a>
 | <a href="#R"><strong>R</strong></a>
 | <a href="#S"><strong>S</strong></a>
 | <a href="#T"><strong>T</strong></a>
 | <a href="#U"><strong>U</strong></a>
 | <a href="#V"><strong>V</strong></a>
 | <a href="#W"><strong>W</strong></a>
 | <a href="#Z"><strong>Z</strong></a>
 
</div>
<h2 id="A">A</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.QuantConfig.activation">activation (torchrec.modules.embedding_configs.QuantConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.ADAGRAD">ADAGRAD (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.ADAM">ADAM (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.ADAMW">ADAMW (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizer.add_param_group">add_param_group() (torchrec.optim.keyed.KeyedOptimizer method)</a>

      <ul>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper.add_param_group">(torchrec.optim.keyed.OptimizerWrapper method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.add_params_from_parameter_sharding">add_params_from_parameter_sharding() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.add_prefix_to_state_dict">add_prefix_to_state_dict() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#id34">algorithm (torchrec.distributed.types.CacheParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams.algorithm">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Pooled_Req">All2All_Pooled_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all2all_pooled_sync">all2all_pooled_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Pooled_Wait">All2All_Pooled_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Seq_Req">All2All_Seq_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait">All2All_Seq_Req_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all2all_sequence_sync">all2all_sequence_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllDenseInfo">All2AllDenseInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllPooledInfo">All2AllPooledInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo">All2AllSequenceInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2Allv_Req">All2Allv_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all2allv_sync">all2allv_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2Allv_Wait">All2Allv_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo">All2AllVInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_gather_base_pooled">all_gather_base_pooled() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_gather_base_sync">all_gather_base_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_gather_into_tensor_backward">all_gather_into_tensor_backward() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_gather_into_tensor_fake">all_gather_into_tensor_fake() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_gather_into_tensor_setup_context">all_gather_into_tensor_setup_context() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_to_all_single_backward">all_to_all_single_backward() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_to_all_single_fake">all_to_all_single_fake() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.all_to_all_single_setup_context">all_to_all_single_setup_context() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBase_Req">AllGatherBase_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBase_Wait">AllGatherBase_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBaseInfo">AllGatherBaseInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.allocate_budget">allocate_budget() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.alltoall_pooled">alltoall_pooled() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.alltoall_sequence">alltoall_sequence() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.alltoallv">alltoallv() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.append_prefix">append_prefix() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.lazy_extension.LazyModuleExtensionMixin.apply">apply() (torchrec.modules.lazy_extension.LazyModuleExtensionMixin method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection.apply_feature_processor">apply_feature_processor() (torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.apply_mc_method_to_jt_dict">apply_mc_method_to_jt_dict() (in module torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.average_threshold_filter">average_threshold_filter() (in module torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.Awaitable">Awaitable (class in torchrec.distributed.types)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="B">B</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#id14">B_global (torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.B_global">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#id15">B_local (torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.B_local">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#id16">B_local_list (torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.B_local_list">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs.backward">backward (torchrec.distributed.types.QuantizedCommCodecs attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Pooled_Req.backward">backward() (torchrec.distributed.comm_ops.All2All_Pooled_Req static method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Pooled_Wait.backward">(torchrec.distributed.comm_ops.All2All_Pooled_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Seq_Req.backward">(torchrec.distributed.comm_ops.All2All_Seq_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.backward">(torchrec.distributed.comm_ops.All2All_Seq_Req_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2Allv_Req.backward">(torchrec.distributed.comm_ops.All2Allv_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2Allv_Wait.backward">(torchrec.distributed.comm_ops.All2Allv_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBase_Req.backward">(torchrec.distributed.comm_ops.AllGatherBase_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBase_Wait.backward">(torchrec.distributed.comm_ops.AllGatherBase_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatter_Req.backward">(torchrec.distributed.comm_ops.ReduceScatter_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatter_Wait.backward">(torchrec.distributed.comm_ops.ReduceScatter_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBase_Req.backward">(torchrec.distributed.comm_ops.ReduceScatterBase_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.backward">(torchrec.distributed.comm_ops.ReduceScatterBase_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterV_Req.backward">(torchrec.distributed.comm_ops.ReduceScatterV_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterV_Wait.backward">(torchrec.distributed.comm_ops.ReduceScatterV_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.backward">(torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.backward">(torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.CommOpGradientScaling.backward">(torchrec.distributed.embedding_lookup.CommOpGradientScaling static method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id5">backward_recat_tensor (torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.backward_recat_tensor">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.bare_named_parameters">bare_named_parameters() (torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding">BaseCwEmbeddingSharding (class in torchrec.distributed.sharding.cw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding">BaseDpEmbeddingSharding (class in torchrec.distributed.sharding.dp_sharding)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig">BaseEmbeddingConfig (class in torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist">BaseEmbeddingDist (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup">BaseEmbeddingLookup (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingSharder">BaseEmbeddingSharder (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.BaseFeatureProcessor">BaseFeatureProcessor (class in torchrec.modules.feature_processor)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor">BaseGroupedFeatureProcessor (class in torchrec.distributed.embedding_types)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.BaseGroupedFeatureProcessor">(class in torchrec.modules.feature_processor)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.BaseManagedCollisionEmbeddingCollection">BaseManagedCollisionEmbeddingCollection (class in torchrec.modules.mc_embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder">BaseQuantEmbeddingSharder (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding">BaseRwEmbeddingSharding (class in torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist">BaseSparseFeaturesDist (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding">BaseTwEmbeddingSharding (class in torchrec.distributed.sharding.tw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding">BaseTwRwEmbeddingSharding (class in torchrec.distributed.sharding.twrw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllDenseInfo.batch_size">batch_size (torchrec.distributed.comm_ops.All2AllDenseInfo attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.batch_size">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id28">batch_size_per_feature_pre_a2a (torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo.batch_size_per_feature_pre_a2a">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#id0">batch_size_per_rank (torchrec.distributed.comm_ops.All2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllPooledInfo.batch_size_per_rank">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#id29">batch_size_per_rank_per_feature (torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo.batch_size_per_rank_per_feature">[1]</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id0">batch_sizes (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.batch_sizes">[1]</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.batching_metadata">batching_metadata() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.batching_metadata_json">batching_metadata_json() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.BatchingMetadata">BatchingMetadata (class in torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.LuusJaakolaSearch.best">best() (torchrec.distributed.planner.utils.LuusJaakolaSearch method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.BinarySearchPredicate">BinarySearchPredicate (class in torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id1">bounds_check_mode (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.bounds_check_mode">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.bounds_check_mode">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.bounds_check_mode">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.InputDistOutputs.bucket_mapping_tensor">bucket_mapping_tensor (torchrec.distributed.embedding_types.InputDistOutputs attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.bucketize_kjt_before_all2all">bucketize_kjt_before_all2all() (in module torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.bucketize_kjt_inference">bucketize_kjt_inference() (in module torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.InputDistOutputs.bucketized_length">bucketized_length (torchrec.distributed.embedding_types.InputDistOutputs attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.build_affine_storage_model">build_affine_storage_model() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Perf.bwd_comms">bwd_comms (torchrec.distributed.planner.types.Perf attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Perf.bwd_compute">bwd_compute (torchrec.distributed.planner.types.Perf attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.bwd_compute_multiplier">bwd_compute_multiplier (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.bytes_to_gb">bytes_to_gb() (in module torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.bytes_to_mb">bytes_to_mb() (in module torchrec.distributed.planner.utils)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="C">C</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.cache_load_factor">cache_load_factor (torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id2">cache_params (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.cache_params">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.cache_params">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.cache_params">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats.cacheability">cacheability (torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats property)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.CacheStatistics.cacheability">(torchrec.distributed.types.CacheStatistics property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams">CacheParams (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.CacheStatistics">CacheStatistics (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NoOpQuantizedCommCodec.calc_quantized_size">calc_quantized_size() (torchrec.distributed.types.NoOpQuantizedCommCodec method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodec.calc_quantized_size">(torchrec.distributed.types.QuantizedCommCodec method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.calculate_pipeline_io_cost">calculate_pipeline_io_cost() (in module torchrec.distributed.planner.shard_estimators)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.calculate_shard_storages">calculate_shard_storages() (in module torchrec.distributed.planner.shard_estimators)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.callbacks">callbacks (torchrec.distributed.dist_data.PooledEmbeddingsAllToAll property)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.callbacks">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable.callbacks">(torchrec.distributed.dist_data.PooledEmbeddingsAwaitable property)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable.callbacks">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll.callbacks">(torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll property)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll.callbacks">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.Awaitable.callbacks">(torchrec.distributed.types.Awaitable property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.check_module_output_dimension">check_module_output_dimension() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.LuusJaakolaSearch.clamp">clamp() (torchrec.distributed.planner.utils.LuusJaakolaSearch method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.clf_to_bytes">clf_to_bytes() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy.coalesce_history_metadata">coalesce_history_metadata() (torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LFU_EvictionPolicy.coalesce_history_metadata">(torchrec.modules.mc_modules.LFU_EvictionPolicy method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LRU_EvictionPolicy.coalesce_history_metadata">(torchrec.modules.mc_modules.LRU_EvictionPolicy method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicy.coalesce_history_metadata">(torchrec.modules.mc_modules.MCHEvictionPolicy method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id1">codecs (torchrec.distributed.comm_ops.All2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllPooledInfo.codecs">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.html#id6">(torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.codecs">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.codecs">(torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBaseInfo.codecs">(torchrec.distributed.comm_ops.AllGatherBaseInfo attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBaseInfo.codecs">(torchrec.distributed.comm_ops.ReduceScatterBaseInfo attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterInfo.codecs">(torchrec.distributed.comm_ops.ReduceScatterInfo attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#id23">(torchrec.distributed.comm_ops.ReduceScatterVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterVInfo.codecs">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#id30">(torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo.codecs">[1]</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.planners.EmbeddingShardingPlanner.collective_plan">collective_plan() (torchrec.distributed.planner.planners.EmbeddingShardingPlanner method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.planners.HeteroEmbeddingShardingPlanner.collective_plan">(torchrec.distributed.planner.planners.HeteroEmbeddingShardingPlanner method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingPlanner.collective_plan">(torchrec.distributed.types.ShardingPlanner method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingType.COLUMN_WISE">COLUMN_WISE (torchrec.distributed.types.ShardingType attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.ShardingType.COLUMN_WISE">(torchrec.modules.embedding_configs.ShardingType attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer">CombinedOptimizer (class in torchrec.optim.keyed)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.CommOp">CommOp (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.CommOpGradientScaling">CommOpGradientScaling (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.compute">compute() (torchrec.distributed.embedding.ShardedEmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.compute">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.compute">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection.compute">(torchrec.distributed.mc_modules.ShardedManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.compute">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection.compute">(torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.compute">(torchrec.distributed.types.ShardedModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.compute_and_output_dist">compute_and_output_dist() (torchrec.distributed.embedding.ShardedEmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.compute_and_output_dist">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.compute_and_output_dist">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.compute_and_output_dist">(torchrec.distributed.types.ShardedModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.compute_device">compute_device (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingAttributes.compute_kernel">compute_kernel (torchrec.distributed.embedding_types.EmbeddingAttributes attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.compute_kernel">(torchrec.distributed.embedding_types.GroupedEmbeddingConfig attribute)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.compute_kernel">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.compute_kernel">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.compute_kernel_to_embedding_location">compute_kernel_to_embedding_location() (in module torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id3">compute_kernels (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.compute_kernels">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingSharder.compute_kernels">compute_kernels() (torchrec.distributed.embedding_types.BaseEmbeddingSharder method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder.compute_kernels">(torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder.compute_kernels">(torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder.compute_kernels">(torchrec.distributed.types.ModuleSharder method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.ComputeJTDictToKJT">ComputeJTDictToKJT (class in torchrec.sparse.jagged_tensor)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ComputeKernel">ComputeKernel (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.ComputeKJTToJTDict">ComputeKJTToJTDict (class in torchrec.sparse.jagged_tensor)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.concat">concat() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor static method)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy.CONSTANT">CONSTANT (torchrec.optim.warmup.WarmupPolicy attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.construct_jagged_tensors">construct_jagged_tensors() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.construct_jagged_tensors_inference">construct_jagged_tensors_inference() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.construct_modulelist_from_single_module">construct_modulelist_from_single_module() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.construct_output_kt">construct_output_kt() (in module torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.convert_list_of_modules_to_modulelist">convert_list_of_modules_to_modulelist() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.convert_to_fbgemm_types">convert_to_fbgemm_types() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.copy">copy() (torchrec.distributed.model_parallel.DistributedModelParallel method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.copy">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.utils.CopyableMixin.copy">(torchrec.distributed.utils.CopyableMixin method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.copy_to_device">copy_to_device() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.CopyableMixin">CopyableMixin (class in torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy.COSINE_ANNEALING_WARM_RESTARTS">COSINE_ANNEALING_WARM_RESTARTS (torchrec.optim.warmup.WarmupPolicy attribute)</a>
</li>
      <li><a href="torchrec.fx.html#torchrec.fx.tracer.Tracer.create_arg">create_arg() (torchrec.fx.tracer.Tracer method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.create_context">create_context() (torchrec.distributed.embedding.ShardedEmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.create_context">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.create_context">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ShardedManagedCollisionEmbeddingCollection.create_context">(torchrec.distributed.mc_embedding.ShardedManagedCollisionEmbeddingCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ShardedManagedCollisionEmbeddingBagCollection.create_context">(torchrec.distributed.mc_embeddingbag.ShardedManagedCollisionEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection.create_context">(torchrec.distributed.mc_modules.ShardedManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.create_context">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.NoOpQuantizedCommCodec.create_context">(torchrec.distributed.types.NoOpQuantizedCommCodec method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodec.create_context">(torchrec.distributed.types.QuantizedCommCodec method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.create_context">(torchrec.distributed.types.ShardedModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.create_embedding_bag_sharding">create_embedding_bag_sharding() (in module torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.create_embedding_sharding">create_embedding_sharding() (in module torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.create_infer_embedding_bag_sharding">create_infer_embedding_bag_sharding() (in module torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.create_input_dist">create_input_dist() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_input_dist">(torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.create_lookup">create_lookup() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_lookup">(torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.create_mc_sharding">create_mc_sharding() (in module torchrec.distributed.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.create_output_dist">create_output_dist() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_output_dist">(torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.create_predict_module">create_predict_module() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.create_sharding_infos_by_group">create_sharding_infos_by_group() (in module torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.create_sharding_infos_by_sharding">create_sharding_infos_by_sharding() (in module torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.create_sharding_infos_by_sharding_device_group">create_sharding_infos_by_sharding_device_group() (in module torchrec.distributed.embedding)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.create_sharding_infos_by_sharding_device_group">(in module torchrec.distributed.embeddingbag)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.crossnet.CrossNet">CrossNet (class in torchrec.modules.crossnet)</a>
</li>
      <li><a href="torchrec.distributed.html#id2">cumsum_dim_sum_per_rank_tensor (torchrec.distributed.comm_ops.All2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllPooledInfo.cumsum_dim_sum_per_rank_tensor">[1]</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.CustomTopologyData">CustomTopologyData (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding">CwPooledEmbeddingSharding (class in torchrec.distributed.sharding.cw_sharding)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="D">D</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#id17">D_local_list (torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.D_local_list">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingType.DATA_PARALLEL">DATA_PARALLEL (torchrec.distributed.types.ShardingType attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.ShardingType.DATA_PARALLEL">(torchrec.modules.embedding_configs.ShardingType attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.data_type">data_type (torchrec.distributed.embedding_types.GroupedEmbeddingConfig attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.data_type">(torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.data_type_to_dtype">data_type_to_dtype() (in module torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.data_type_to_sparse_type">data_type_to_sparse_type() (in module torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DataParallelWrapper">DataParallelWrapper (class in torchrec.distributed.model_parallel)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Storage.ddr">ddr (torchrec.distributed.planner.types.Storage attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterStorage.DDR">DDR (torchrec.distributed.types.ParameterStorage attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.ddr_mem_bw">ddr_mem_bw (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupStage.decay_iters">decay_iters (torchrec.optim.warmup.WarmupStage attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NoOpQuantizedCommCodec.decode">decode() (torchrec.distributed.types.NoOpQuantizedCommCodec method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodec.decode">(torchrec.distributed.types.QuantizedCommCodec method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.deepfm.DeepFM">DeepFM (class in torchrec.modules.deepfm)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ComputeKernel.DEFAULT">DEFAULT (torchrec.distributed.types.ComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DefaultDataParallelWrapper">DefaultDataParallelWrapper (class in torchrec.distributed.model_parallel)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.DENSE">DENSE (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.models.html#torchrec.models.deepfm.DenseArch">DenseArch (class in torchrec.models.deepfm)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.dependency">dependency (torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.deterministic_dedup">deterministic_dedup() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.device">device (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.OrderedDeviceHardware.device">(torchrec.distributed.planner.partitioners.OrderedDeviceHardware attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PartitionByType.DEVICE">DEVICE (torchrec.distributed.planner.types.PartitionByType attribute)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.BatchingMetadata.device">device (torchrec.inference.modules.BatchingMetadata attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection.device">(torchrec.modules.embedding_modules.EmbeddingBagCollection property)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.device">(torchrec.modules.embedding_modules.EmbeddingCollection property)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.device">(torchrec.modules.mc_modules.ManagedCollisionModule property)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection.device">(torchrec.quant.embedding_modules.EmbeddingBagCollection property)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.device">(torchrec.quant.embedding_modules.EmbeddingCollection property)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.device">device() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.device">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.device">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#id4">device_group (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.device_group">[1]</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.DeviceHardware">DeviceHardware (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.devices">devices (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.dim_sum">dim_sum() (torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>
</li>
      <li><a href="torchrec.distributed.html#id3">dim_sum_per_rank (torchrec.distributed.comm_ops.All2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllPooledInfo.dim_sum_per_rank">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.dim_sum_per_rank">(torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id4">dim_sum_per_rank_tensor (torchrec.distributed.comm_ops.All2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllPooledInfo.dim_sum_per_rank_tensor">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.dims_sum_per_rank">dims_sum_per_rank (torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.dist_init">dist_init() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor static method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.dist_labels">dist_labels() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.dist_splits">dist_splits() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.dist_tensors">dist_tensors() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy">DistanceLFU_EvictionPolicy (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel">DistributedModelParallel (class in torchrec.distributed.model_parallel)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext.divisor">divisor (torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext attribute)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist">DpPooledEmbeddingDist (class in torchrec.distributed.sharding.dp_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding">DpPooledEmbeddingSharding (class in torchrec.distributed.sharding.dp_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist">DpSparseFeaturesDist (class in torchrec.distributed.sharding.dp_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedMetaConfig.dtensor_metadata">dtensor_metadata (torchrec.distributed.embedding_types.ShardedMetaConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.DTensorMetadata">DTensorMetadata (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.dtype_to_data_type">dtype_to_data_type() (in module torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.dummy_tensor">dummy_tensor() (in module torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.dynamic_threshold_filter">dynamic_threshold_filter() (in module torchrec.modules.mc_modules)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="E">E</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#id31">emb_dim_per_rank_per_feature (torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo.emb_dim_per_rank_per_feature">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.embedding_bag_configs">embedding_bag_configs() (torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection.embedding_bag_configs">(torchrec.modules.embedding_modules.EmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface.embedding_bag_configs">(torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection.embedding_bag_configs">(torchrec.quant.embedding_modules.EmbeddingBagCollection method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection.embedding_bags">embedding_bags (torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection attribute)</a>

      <ul>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection.embedding_bags">(torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo.embedding_config">embedding_config (torchrec.distributed.embedding_sharding.EmbeddingShardingInfo attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.embedding_configs">embedding_configs() (torchrec.modules.embedding_modules.EmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollectionInterface.embedding_configs">(torchrec.modules.embedding_modules.EmbeddingCollectionInterface method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection.embedding_configs">(torchrec.modules.mc_modules.ManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.embedding_configs">(torchrec.quant.embedding_modules.EmbeddingCollection method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id7">embedding_dim (torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.embedding_dim">[1]</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.embedding_dim">(torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingConfig.embedding_dim">(torchrec.modules.embedding_configs.EmbeddingConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.embedding_dim">embedding_dim() (torchrec.modules.embedding_modules.EmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollectionInterface.embedding_dim">(torchrec.modules.embedding_modules.EmbeddingCollectionInterface method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.embedding_dim">(torchrec.quant.embedding_modules.EmbeddingCollection method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.embedding_dims">embedding_dims() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.embedding_dims">(torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_dims">(torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_dims">(torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_dims">(torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_dims">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_dims">(torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig.embedding_names">embedding_names (torchrec.modules.embedding_configs.EmbeddingTableConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.embedding_names">embedding_names() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.embedding_names">(torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_names">(torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names">(torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names">(torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names">(torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.embedding_names_by_table">embedding_names_by_table() (torchrec.modules.embedding_modules.EmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollectionInterface.embedding_names_by_table">(torchrec.modules.embedding_modules.EmbeddingCollectionInterface method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.embedding_names_by_table">(torchrec.quant.embedding_modules.EmbeddingCollection method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.embedding_names_per_rank">embedding_names_per_rank() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names_per_rank">(torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names_per_rank">(torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names_per_rank">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names_per_rank">(torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.embedding_shard_metadata">embedding_shard_metadata() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.embedding_shard_metadata">(torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_shard_metadata">(torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_shard_metadata">(torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_shard_metadata">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_shard_metadata">(torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.embedding_tables">embedding_tables (torchrec.distributed.embedding_types.GroupedEmbeddingConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.embedding_tables">embedding_tables() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_tables">(torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_tables">(torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_tables">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingAttributes">EmbeddingAttributes (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingAwaitable">EmbeddingAwaitable (class in torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection">EmbeddingBagCollection (class in torchrec.modules.embedding_modules)</a>

      <ul>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection">(class in torchrec.quant.embedding_modules)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionAwaitable">EmbeddingBagCollectionAwaitable (class in torchrec.distributed.embeddingbag)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext">EmbeddingBagCollectionContext (class in torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface">EmbeddingBagCollectionInterface (class in torchrec.modules.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder">EmbeddingBagCollectionSharder (class in torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingBagConfig">EmbeddingBagConfig (class in torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagSharder">EmbeddingBagSharder (class in torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection">EmbeddingCollection (class in torchrec.modules.embedding_modules)</a>

      <ul>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection">(class in torchrec.quant.embedding_modules)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionAwaitable">EmbeddingCollectionAwaitable (class in torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionContext">EmbeddingCollectionContext (class in torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollectionInterface">EmbeddingCollectionInterface (class in torchrec.modules.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionSharder">EmbeddingCollectionSharder (class in torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel">EmbeddingComputeKernel (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingConfig">EmbeddingConfig (class in torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.enumerators.EmbeddingEnumerator">EmbeddingEnumerator (class in torchrec.distributed.planner.enumerators)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.EmbeddingModuleShardingPlan">EmbeddingModuleShardingPlan (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer">EmbeddingOffloadScaleupProposer (class in torchrec.distributed.planner.proposers)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats">EmbeddingOffloadStats (class in torchrec.distributed.planner.shard_estimators)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator">EmbeddingPerfEstimator (class in torchrec.distributed.planner.shard_estimators)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.embeddings_cat_empty_rank_handle">embeddings_cat_empty_rank_handle() (in module torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.embeddings_cat_empty_rank_handle_inference">embeddings_cat_empty_rank_handle_inference() (in module torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOne">EmbeddingsAllToOne (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOne">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce">EmbeddingsAllToOneReduce (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding">EmbeddingSharding (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext">EmbeddingShardingContext (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo">EmbeddingShardingInfo (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.planners.EmbeddingShardingPlanner">EmbeddingShardingPlanner (class in torchrec.distributed.planner.planners)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.stats.EmbeddingStats">EmbeddingStats (class in torchrec.distributed.planner.stats)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator">EmbeddingStorageEstimator (class in torchrec.distributed.planner.shard_estimators)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig">EmbeddingTableConfig (class in torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.empty">empty() (torchrec.sparse.jagged_tensor.JaggedTensor static method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.empty">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor static method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.empty_like">empty_like() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor static method)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.fused.EmptyFusedOptimizer">EmptyFusedOptimizer (class in torchrec.optim.fused)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NoOpQuantizedCommCodec.encode">encode() (torchrec.distributed.types.NoOpQuantizedCommCodec method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodec.encode">(torchrec.distributed.types.QuantizedCommCodec method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#id5">enforce_hbm (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.enforce_hbm">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.enforce_hbm">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.enforce_hbm">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.enumerators.EmbeddingEnumerator.enumerate">enumerate() (torchrec.distributed.planner.enumerators.EmbeddingEnumerator method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Enumerator.enumerate">(torchrec.distributed.planner.types.Enumerator method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Enumerator">Enumerator (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.html#id24">equal_splits (torchrec.distributed.comm_ops.ReduceScatterVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterVInfo.equal_splits">[1]</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.estimate">estimate() (torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator.estimate">(torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardEstimator.estimate">(torchrec.distributed.planner.types.ShardEstimator method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats.estimate_cache_miss_rate">estimate_cache_miss_rate() (torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats static method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.evict">evict() (in module torchrec.modules.mc_embedding_modules)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection.evict">(torchrec.distributed.mc_modules.ShardedManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection.evict">(torchrec.modules.mc_modules.ManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.evict">(torchrec.modules.mc_modules.ManagedCollisionModule method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.evict">(torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext.evictions_per_table">evictions_per_table (torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats.expected_lookups">expected_lookups (torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats property)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.CacheStatistics.expected_lookups">(torchrec.distributed.types.CacheStatistics property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats.expected_miss_rate">expected_miss_rate() (torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.CacheStatistics.expected_miss_rate">(torchrec.distributed.types.CacheStatistics method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedEmbeddingModule.extra_repr">extra_repr() (torchrec.distributed.embedding_types.ShardedEmbeddingModule method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.extract_module_or_tensor_callable">extract_module_or_tensor_callable() (in module torchrec.modules.utils)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="F">F</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.modules.html#torchrec.modules.deepfm.FactorizationMachine">FactorizationMachine (class in torchrec.modules.deepfm)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.feature_hash_sizes">feature_hash_sizes() (torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id6">feature_names (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.feature_names">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.feature_names">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.feature_names">(torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingConfig.feature_names">(torchrec.modules.embedding_configs.EmbeddingConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.FeatureShardingMixIn.feature_names">feature_names() (torchrec.distributed.embedding_types.FeatureShardingMixIn method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.feature_names">(torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.feature_names">(torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.feature_names">(torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.feature_names">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.feature_names">(torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.FeatureShardingMixIn.feature_names_per_rank">feature_names_per_rank() (torchrec.distributed.embedding_types.FeatureShardingMixIn method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.feature_names_per_rank">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection">FeatureProcessedEmbeddingBagCollection (class in torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.InputDistOutputs.features">features (torchrec.distributed.embedding_types.InputDistOutputs attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.FeatureShardingMixIn.features_per_rank">features_per_rank() (torchrec.distributed.embedding_types.FeatureShardingMixIn method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.features_per_rank">(torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.FeatureShardingMixIn">FeatureShardingMixIn (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.feedback">feedback() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GreedyProposer.feedback">(torchrec.distributed.planner.proposers.GreedyProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GridSearchProposer.feedback">(torchrec.distributed.planner.proposers.GridSearchProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.UniformProposer.feedback">(torchrec.distributed.planner.proposers.UniformProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Proposer.feedback">(torchrec.distributed.planner.types.Proposer method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.filter_state_dict">filter_state_dict() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Storage.fits_in">fits_in() (torchrec.distributed.planner.types.Storage method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.FixedPercentageStorageReservation">FixedPercentageStorageReservation (class in torchrec.distributed.planner.storage_reservations)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.flatten_feature_lengths">flatten_feature_lengths() (in module torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.flatten_kjt_list">flatten_kjt_list() (in module torchrec.sparse.jagged_tensor)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.flatten_lengths">flatten_lengths() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.flush">flush() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.flush">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.flush">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.flush">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.models.html#torchrec.models.deepfm.FMInteractionArch">FMInteractionArch (class in torchrec.models.deepfm)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.for_each_module_of_type_do">for_each_module_of_type_do() (in module torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.ForkedPdb">ForkedPdb (class in torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs.forward">forward (torchrec.distributed.types.QuantizedCommCodecs attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward">forward() (torchrec.distributed.comm_ops.All2All_Pooled_Req static method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward">(torchrec.distributed.comm_ops.All2All_Pooled_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Seq_Req.forward">(torchrec.distributed.comm_ops.All2All_Seq_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward">(torchrec.distributed.comm_ops.All2All_Seq_Req_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2Allv_Req.forward">(torchrec.distributed.comm_ops.All2Allv_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2Allv_Wait.forward">(torchrec.distributed.comm_ops.All2Allv_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBase_Req.forward">(torchrec.distributed.comm_ops.AllGatherBase_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward">(torchrec.distributed.comm_ops.AllGatherBase_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatter_Req.forward">(torchrec.distributed.comm_ops.ReduceScatter_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward">(torchrec.distributed.comm_ops.ReduceScatter_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward">(torchrec.distributed.comm_ops.ReduceScatterBase_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward">(torchrec.distributed.comm_ops.ReduceScatterBase_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward">(torchrec.distributed.comm_ops.ReduceScatterV_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward">(torchrec.distributed.comm_ops.ReduceScatterV_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward">(torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward">(torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOne.forward">(torchrec.distributed.dist_data.EmbeddingsAllToOne method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOne.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce.forward">(torchrec.distributed.dist_data.EmbeddingsAllToOneReduce method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTAllToAll.forward">(torchrec.distributed.dist_data.KJTAllToAll method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAll.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTOneToAll.forward">(torchrec.distributed.dist_data.KJTOneToAll method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTOneToAll.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.MergePooledEmbeddingsModule.forward">(torchrec.distributed.dist_data.MergePooledEmbeddingsModule method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.MergePooledEmbeddingsModule.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.forward">(torchrec.distributed.dist_data.PooledEmbeddingsAllGather method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.forward">(torchrec.distributed.dist_data.PooledEmbeddingsAllToAll method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.forward">(torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.forward">(torchrec.distributed.dist_data.SeqEmbeddingsAllToOne method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.forward">(torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorAllToAll.forward">(torchrec.distributed.dist_data.TensorAllToAll method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorAllToAll.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorValuesAllToAll.forward">(torchrec.distributed.dist_data.TensorValuesAllToAll method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorValuesAllToAll.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll.forward">(torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter.forward">(torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter.forward">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward">(torchrec.distributed.embedding_lookup.CommOpGradientScaling static method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.forward">(torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.forward">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin.forward">(torchrec.distributed.embedding_lookup.InferGroupedLookupMixin method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.forward">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.forward">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist.forward">(torchrec.distributed.embedding_sharding.BaseEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist.forward">(torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup.forward">(torchrec.distributed.embedding_types.BaseEmbeddingLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor.forward">(torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule.forward">(torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.forward">(torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEbcInputDist.forward">(torchrec.distributed.quant_embeddingbag.ShardedQuantEbcInputDist method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.forward">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDist.forward">(torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDistWithPermute.forward">(torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDistWithPermute method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.forward">(torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.forward">(torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingDist.forward">(torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwSparseFeaturesDist.forward">(torchrec.distributed.sharding.rw_sharding.InferRwSparseFeaturesDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.forward">(torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.forward">(torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.forward">(torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.forward">(torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.forward">(torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.forward">(torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.forward">(torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist method)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.forward">(torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.forward">(torchrec.distributed.types.ShardedModule method)</a>
</li>
        <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictModule.forward">(torchrec.inference.modules.PredictModule method)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.DenseArch.forward">(torchrec.models.deepfm.DenseArch method)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.FMInteractionArch.forward">(torchrec.models.deepfm.FMInteractionArch method)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.OverArch.forward">(torchrec.models.deepfm.OverArch method)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.SimpleDeepFMNN.forward">(torchrec.models.deepfm.SimpleDeepFMNN method)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.SparseArch.forward">(torchrec.models.deepfm.SparseArch method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.activation.SwishLayerNorm.forward">(torchrec.modules.activation.SwishLayerNorm method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.CrossNet.forward">(torchrec.modules.crossnet.CrossNet method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.LowRankCrossNet.forward">(torchrec.modules.crossnet.LowRankCrossNet method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.LowRankMixtureCrossNet.forward">(torchrec.modules.crossnet.LowRankMixtureCrossNet method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.VectorCrossNet.forward">(torchrec.modules.crossnet.VectorCrossNet method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.deepfm.DeepFM.forward">(torchrec.modules.deepfm.DeepFM method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.deepfm.FactorizationMachine.forward">(torchrec.modules.deepfm.FactorizationMachine method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection.forward">(torchrec.modules.embedding_modules.EmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface.forward">(torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.forward">(torchrec.modules.embedding_modules.EmbeddingCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollectionInterface.forward">(torchrec.modules.embedding_modules.EmbeddingCollectionInterface method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.BaseFeatureProcessor.forward">(torchrec.modules.feature_processor.BaseFeatureProcessor method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.BaseGroupedFeatureProcessor.forward">(torchrec.modules.feature_processor.BaseGroupedFeatureProcessor method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedModule.forward">(torchrec.modules.feature_processor.PositionWeightedModule method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedProcessor.forward">(torchrec.modules.feature_processor.PositionWeightedProcessor method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.BaseManagedCollisionEmbeddingCollection.forward">(torchrec.modules.mc_embedding_modules.BaseManagedCollisionEmbeddingCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection.forward">(torchrec.modules.mc_modules.ManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.forward">(torchrec.modules.mc_modules.ManagedCollisionModule method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.forward">(torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mlp.MLP.forward">(torchrec.modules.mlp.MLP method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mlp.Perceptron.forward">(torchrec.modules.mlp.Perceptron method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection.forward">(torchrec.quant.embedding_modules.EmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.forward">(torchrec.quant.embedding_modules.EmbeddingCollection method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection.forward">(torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.ComputeJTDictToKJT.forward">(torchrec.sparse.jagged_tensor.ComputeJTDictToKJT method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.ComputeKJTToJTDict.forward">(torchrec.sparse.jagged_tensor.ComputeKJTToJTDict method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#id8">forward_recat_tensor (torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.forward_recat_tensor">[1]</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.fqn">fqn (torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.from_dense">from_dense() (torchrec.sparse.jagged_tensor.JaggedTensor static method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.from_dense_lengths">from_dense_lengths() (torchrec.sparse.jagged_tensor.JaggedTensor static method)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection.from_float">from_float() (torchrec.quant.embedding_modules.EmbeddingBagCollection class method)</a>

      <ul>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.from_float">(torchrec.quant.embedding_modules.EmbeddingCollection class method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection.from_float">(torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection class method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.from_jt_dict">from_jt_dict() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor static method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.from_lengths_sync">from_lengths_sync() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor static method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv.from_local">from_local() (torchrec.distributed.types.ShardingEnv class method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.from_offsets_sync">from_offsets_sync() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor static method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv.from_process_group">from_process_group() (torchrec.distributed.types.ShardingEnv class method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.from_tensor_list">from_tensor_list() (torchrec.sparse.jagged_tensor.KeyedTensor static method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.FUSED">FUSED (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.fused_optimizer">fused_optimizer (torchrec.distributed.embedding.ShardedEmbeddingCollection property)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.fused_optimizer">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.fused_optimizer">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.fused_optimizer">(torchrec.distributed.model_parallel.DistributedModelParallel property)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule.fused_optimizer">(torchrec.optim.fused.FusedOptimizerModule property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo.fused_params">fused_params (torchrec.distributed.embedding_sharding.EmbeddingShardingInfo attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingSharder.fused_params">(torchrec.distributed.embedding_types.BaseEmbeddingSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder.fused_params">(torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.fused_params">(torchrec.distributed.embedding_types.GroupedEmbeddingConfig attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedEmbeddingTable.fused_params">(torchrec.distributed.embedding_types.ShardedEmbeddingTable attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.FUSED_UVM">FUSED_UVM (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.FUSED_UVM_CACHING">FUSED_UVM_CACHING (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.FusedKJTListSplitsAwaitable">FusedKJTListSplitsAwaitable (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.fused.FusedOptimizer">FusedOptimizer (class in torchrec.optim.fused)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule">FusedOptimizerModule (class in torchrec.optim.fused)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Perf.fwd_comms">fwd_comms (torchrec.distributed.planner.types.Perf attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Perf.fwd_compute">fwd_compute (torchrec.distributed.planner.types.Perf attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.fx_wrap_tensor_view2d">fx_wrap_tensor_view2d() (in module torchrec.distributed.embedding_lookup)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="G">G</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.gb_to_bytes">gb_to_bytes() (in module torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.GenericMeta">GenericMeta (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.get_block_sizes_runtime_device">get_block_sizes_runtime_device() (in module torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.get_budget">get_budget() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.get_cacheability">get_cacheability() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.CustomTopologyData.get_data">get_data() (torchrec.distributed.planner.types.CustomTopologyData method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.get_device_from_parameter_sharding">get_device_from_parameter_sharding() (in module torchrec.distributed.embedding)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.get_device_from_parameter_sharding">(in module torchrec.distributed.embeddingbag)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.get_device_from_parameter_sharding">(in module torchrec.distributed.quant_embeddingbag)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.get_device_from_sharding_infos">get_device_from_sharding_infos() (in module torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.get_ec_index_dedup">get_ec_index_dedup() (in module torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.get_embedding_names_by_table">get_embedding_names_by_table() (in module torchrec.modules.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.get_embedding_shard_metadata">get_embedding_shard_metadata() (in module torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.get_expected_lookups">get_expected_lookups() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.get_gradient_division">get_gradient_division() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm.get_group_rank">get_group_rank() (in module torchrec.distributed.comm)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm.get_local_rank">get_local_rank() (in module torchrec.distributed.comm)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm.get_local_size">get_local_size() (in module torchrec.distributed.comm)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.get_module">get_module() (in module torchrec.distributed.model_parallel)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.get_module_output_dimension">get_module_output_dimension() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm.get_num_groups">get_num_groups() (in module torchrec.distributed.comm)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.enumerators.get_partition_by_type">get_partition_by_type() (in module torchrec.distributed.planner.enumerators)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingPlan.get_plan_for_module">get_plan_for_module() (torchrec.distributed.types.ShardingPlan method)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.get_sharding_group">get_sharding_group() (in module torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferCPUGroupedEmbeddingsLookup.get_tbes_to_register">get_tbes_to_register() (torchrec.distributed.embedding_lookup.InferCPUGroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedEmbeddingsLookup.get_tbes_to_register">(torchrec.distributed.embedding_lookup.InferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedPooledEmbeddingsLookup.get_tbes_to_register">(torchrec.distributed.embedding_lookup.InferGroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.get_tbes_to_register">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.get_tbes_to_register">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.get_tensor_size_bytes">get_tensor_size_bytes() (in module torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.get_unsharded_module_names">get_unsharded_module_names() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.get_unwrapped_module">get_unwrapped_module() (in module torchrec.distributed.model_parallel)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.get_use_sync_collectives">get_use_sync_collectives() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.get_weight_init_max">get_weight_init_max() (torchrec.modules.embedding_configs.BaseEmbeddingConfig method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.get_weight_init_min">get_weight_init_min() (torchrec.modules.embedding_configs.BaseEmbeddingConfig method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.GetItemLazyAwaitable">GetItemLazyAwaitable (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedMetaConfig.global_metadata">global_metadata (torchrec.distributed.embedding_types.ShardedMetaConfig attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.clipping.GradientClipping">GradientClipping (class in torchrec.optim.clipping)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.clipping.GradientClippingOptimizer">GradientClippingOptimizer (class in torchrec.optim.clipping)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.GreedyPerfPartitioner">GreedyPerfPartitioner (class in torchrec.distributed.planner.partitioners)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GreedyProposer">GreedyProposer (class in torchrec.distributed.planner.proposers)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GridSearchProposer">GridSearchProposer (class in torchrec.distributed.planner.proposers)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.group_tables">group_tables() (in module torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig">GroupedEmbeddingConfig (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup">GroupedEmbeddingsLookup (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup">GroupedPooledEmbeddingsLookup (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule">GroupedPositionWeightedModule (class in torchrec.distributed.grouped_position_weighted)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="H">H</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.CustomTopologyData.has_data">has_data() (torchrec.distributed.planner.types.CustomTopologyData method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.has_feature_processor">has_feature_processor (torchrec.distributed.embedding_types.GroupedEmbeddingConfig attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig.has_feature_processor">(torchrec.modules.embedding_configs.EmbeddingTableConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Storage.hbm">hbm (torchrec.distributed.planner.types.Storage attribute)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterStorage.HBM">HBM (torchrec.distributed.types.ParameterStorage attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.hbm_mem_bw">hbm_mem_bw (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.planners.HeteroEmbeddingShardingPlanner">HeteroEmbeddingShardingPlanner (class in torchrec.distributed.planner.planners)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation">HeuristicalStorageReservation (class in torchrec.distributed.planner.storage_reservations)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PartitionByType.HOST">HOST (torchrec.distributed.planner.types.PartitionByType attribute)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="I">I</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.index_per_key">index_per_key() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferCPUGroupedEmbeddingsLookup">InferCPUGroupedEmbeddingsLookup (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDist">InferCwPooledEmbeddingDist (class in torchrec.distributed.sharding.cw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDistWithPermute">InferCwPooledEmbeddingDistWithPermute (class in torchrec.distributed.sharding.cw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding">InferCwPooledEmbeddingSharding (class in torchrec.distributed.sharding.cw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ObjectPoolShardingPlan.inference">inference (torchrec.distributed.types.ObjectPoolShardingPlan attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.InferenceStorageReservation">InferenceStorageReservation (class in torchrec.distributed.planner.storage_reservations)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedEmbeddingsLookup">InferGroupedEmbeddingsLookup (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin">InferGroupedLookupMixin (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedPooledEmbeddingsLookup">InferGroupedPooledEmbeddingsLookup (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingDist">InferRwPooledEmbeddingDist (class in torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding">InferRwPooledEmbeddingSharding (class in torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwSparseFeaturesDist">InferRwSparseFeaturesDist (class in torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding">InferTwEmbeddingSharding (class in torchrec.distributed.sharding.tw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist">InferTwPooledEmbeddingDist (class in torchrec.distributed.sharding.tw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist">InferTwSparseFeaturesDist (class in torchrec.distributed.sharding.tw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.init_data_parallel">init_data_parallel() (torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.init_fn">init_fn (torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.init_mlp_weights_xavier_uniform">init_mlp_weights_xavier_uniform() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.init_parameters">init_parameters() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizer.init_state">init_state() (torchrec.optim.keyed.KeyedOptimizer method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.input_dist">input_dist() (torchrec.distributed.embedding.ShardedEmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.input_dist">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.input_dist">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection.input_dist">(torchrec.distributed.mc_modules.ShardedManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.input_dist">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.input_dist">(torchrec.distributed.types.ShardedModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.input_lengths">input_lengths (torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllDenseInfo.input_shape">input_shape (torchrec.distributed.comm_ops.All2AllDenseInfo attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#id20">input_size (torchrec.distributed.comm_ops.AllGatherBaseInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.AllGatherBaseInfo.input_size">[1]</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.input_size">input_size() (torchrec.modules.mc_modules.ManagedCollisionModule method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.input_size">(torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id21">input_sizes (torchrec.distributed.comm_ops.ReduceScatterBaseInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBaseInfo.input_sizes">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.html#id22">(torchrec.distributed.comm_ops.ReduceScatterInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterInfo.input_sizes">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#id25">(torchrec.distributed.comm_ops.ReduceScatterVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterVInfo.input_sizes">[1]</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#id18">input_split_sizes (torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.input_split_sizes">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllDenseInfo.input_splits">input_splits (torchrec.distributed.comm_ops.All2AllDenseInfo attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.html#id9">(torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.input_splits">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#id26">(torchrec.distributed.comm_ops.ReduceScatterVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterVInfo.input_splits">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#id32">(torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo.input_splits">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.input_splits">(torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.input_tensors">input_tensors (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.InputDistOutputs">InputDistOutputs (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PlannerErrorType.INSUFFICIENT_STORAGE">INSUFFICIENT_STORAGE (torchrec.distributed.planner.types.PlannerErrorType attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.inter_host_bw">inter_host_bw (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.ForkedPdb.interaction">interaction() (torchrec.distributed.utils.ForkedPdb method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm.intra_and_cross_node_pg">intra_and_cross_node_pg() (in module torchrec.distributed.comm)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.intra_host_bw">intra_host_bw (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext.inverse_indices">inverse_indices (torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext attribute)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.inverse_indices">inverse_indices() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.inverse_indices_or_none">inverse_indices_or_none() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.collective_utils.invoke_on_rank_and_broadcast_result">invoke_on_rank_and_broadcast_result() (in module torchrec.distributed.collective_utils)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy.INVSQRT">INVSQRT (torchrec.optim.warmup.WarmupPolicy attribute)</a>
</li>
      <li><a href="torchrec.fx.html#torchrec.fx.tracer.is_fx_tracing">is_fx_tracing() (in module torchrec.fx.tracer)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo.is_history_metadata">is_history_metadata (torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.collective_utils.is_leader">is_leader() (in module torchrec.distributed.collective_utils)</a>
</li>
      <li><a href="torchrec.fx.html#torchrec.fx.tracer.Tracer.is_leaf_module">is_leaf_module() (torchrec.fx.tracer.Tracer method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo.is_mch_metadata">is_mch_metadata (torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.is_pooled">is_pooled (torchrec.distributed.planner.types.ShardingOption attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#id15">(torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.is_weighted">is_weighted (torchrec.distributed.embedding_types.GroupedEmbeddingConfig attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#id7">(torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.is_weighted">[1]</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig.is_weighted">(torchrec.modules.embedding_configs.EmbeddingTableConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection.is_weighted">is_weighted() (torchrec.modules.embedding_modules.EmbeddingBagCollection method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface.is_weighted">(torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection.is_weighted">(torchrec.quant.embedding_modules.EmbeddingBagCollection method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="J">J</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.jagged_index_select_with_empty">jagged_index_select_with_empty() (in module torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor">JaggedTensor (class in torchrec.sparse.jagged_tensor)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.JaggedTensorAllToAll">JaggedTensorAllToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.JaggedTensorAllToAll">[1]</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensorMeta">JaggedTensorMeta (class in torchrec.sparse.jagged_tensor)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.jt_is_equal">jt_is_equal() (in module torchrec.sparse.jagged_tensor)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="K">K</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.constants.kernel_bw_lookup">kernel_bw_lookup() (in module torchrec.distributed.planner.constants)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.key_dim">key_dim() (torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.KEY_VALUE">KEY_VALUE (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id8">key_value_params (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.key_value_params">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.key_value_params">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.key_value_params">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor">KeyedJaggedTensor (class in torchrec.sparse.jagged_tensor)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizer">KeyedOptimizer (class in torchrec.optim.keyed)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizerWrapper">KeyedOptimizerWrapper (class in torchrec.optim.keyed)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor">KeyedTensor (class in torchrec.sparse.jagged_tensor)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.keys">keys (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.keys">keys() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.keys">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.KeyValueParams">KeyValueParams (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.kjt_is_equal">kjt_is_equal() (in module torchrec.sparse.jagged_tensor)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTAllToAll">KJTAllToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAll">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTAllToAllSplitsAwaitable">KJTAllToAllSplitsAwaitable (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllSplitsAwaitable">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable">KJTAllToAllTensorsAwaitable (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList">KJTList (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTListAwaitable">KJTListAwaitable (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTListSplitsAwaitable">KJTListSplitsAwaitable (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTOneToAll">KJTOneToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTOneToAll">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta">KJTSplitsAllToAllMeta (class in torchrec.distributed.embedding_sharding)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="L">L</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.labels">labels (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.LAMB">LAMB (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.LARS_SGD">LARS_SGD (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.lazy_extension.lazy_apply">lazy_apply() (in module torchrec.modules.lazy_extension)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.LazyAwaitable">LazyAwaitable (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.LazyGetItemMixin">LazyGetItemMixin (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.lazy_extension.LazyModuleExtensionMixin">LazyModuleExtensionMixin (class in torchrec.modules.lazy_extension)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.LazyNoWait">LazyNoWait (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.length_per_key">length_per_key() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.length_per_key">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.length_per_key_or_none">length_per_key_or_none() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.lengths">lengths() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.lengths">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id10">lengths_after_sparse_data_all2all (torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.lengths_after_sparse_data_all2all">[1]</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.lengths_offset_per_key">lengths_offset_per_key() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.lengths_or_none">lengths_or_none() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.lengths_or_none">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LFU_EvictionPolicy">LFU_EvictionPolicy (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy.LINEAR">LINEAR (torchrec.optim.warmup.WarmupPolicy attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.LION">LION (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ListOfKJTList">ListOfKJTList (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.ListOfKJTListAwaitable">ListOfKJTListAwaitable (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.ListOfKJTListSplitsAwaitable">ListOfKJTListSplitsAwaitable (class in torchrec.distributed.embedding_sharding)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.load">load() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GreedyProposer.load">(torchrec.distributed.planner.proposers.GreedyProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GridSearchProposer.load">(torchrec.distributed.planner.proposers.GridSearchProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.UniformProposer.load">(torchrec.distributed.planner.proposers.UniformProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Proposer.load">(torchrec.distributed.planner.types.Proposer method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.inference.html#torchrec.inference.model_packager.load_config_text">load_config_text() (in module torchrec.inference.model_packager)</a>
</li>
      <li><a href="torchrec.distributed.html#id35">load_factor (torchrec.distributed.types.CacheParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams.load_factor">[1]</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.model_packager.load_pickle_config">load_pickle_config() (in module torchrec.inference.model_packager)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict">load_state_dict() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin.load_state_dict">(torchrec.distributed.embedding_lookup.InferGroupedLookupMixin method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict">(torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizer.load_state_dict">(torchrec.optim.keyed.KeyedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper.load_state_dict">(torchrec.optim.keyed.OptimizerWrapper method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedConfig.local_cols">local_cols (torchrec.distributed.embedding_types.ShardedConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedMetaConfig.local_metadata">local_metadata (torchrec.distributed.embedding_types.ShardedMetaConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedConfig.local_rows">local_rows (torchrec.distributed.embedding_types.ShardedConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.OrderedDeviceHardware.local_world_size">local_world_size (torchrec.distributed.planner.partitioners.OrderedDeviceHardware attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.local_world_size">(torchrec.distributed.planner.types.Topology property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.stats.EmbeddingStats.log">log() (torchrec.distributed.planner.stats.EmbeddingStats method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.stats.NoopEmbeddingStats.log">(torchrec.distributed.planner.stats.NoopEmbeddingStats method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Stats.log">(torchrec.distributed.planner.types.Stats method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.crossnet.LowRankCrossNet">LowRankCrossNet (class in torchrec.modules.crossnet)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.crossnet.LowRankMixtureCrossNet">LowRankMixtureCrossNet (class in torchrec.modules.crossnet)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupStage.lr_scale">lr_scale (torchrec.optim.warmup.WarmupStage attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LRU_EvictionPolicy">LRU_EvictionPolicy (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.LuusJaakolaSearch">LuusJaakolaSearch (class in torchrec.distributed.planner.utils)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="M">M</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection">ManagedCollisionCollection (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ManagedCollisionCollectionAwaitable">ManagedCollisionCollectionAwaitable (class in torchrec.distributed.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ManagedCollisionCollectionContext">ManagedCollisionCollectionContext (class in torchrec.distributed.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder">ManagedCollisionCollectionSharder (class in torchrec.distributed.mc_modules)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection">ManagedCollisionEmbeddingBagCollection (class in torchrec.modules.mc_embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext">ManagedCollisionEmbeddingBagCollectionContext (class in torchrec.distributed.mc_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionSharder">ManagedCollisionEmbeddingBagCollectionSharder (class in torchrec.distributed.mc_embeddingbag)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection">ManagedCollisionEmbeddingCollection (class in torchrec.modules.mc_embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext">ManagedCollisionEmbeddingCollectionContext (class in torchrec.distributed.mc_embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionSharder">ManagedCollisionEmbeddingCollectionSharder (class in torchrec.distributed.mc_embedding)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule">ManagedCollisionModule (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupStage.max_iters">max_iters (torchrec.optim.warmup.WarmupStage attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicy">MCHEvictionPolicy (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo">MCHEvictionPolicyMetadataInfo (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule">MCHManagedCollisionModule (class in torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.PoolingType.MEAN">MEAN (torchrec.modules.embedding_configs.PoolingType attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.MemoryBalancedPartitioner">MemoryBalancedPartitioner (class in torchrec.distributed.planner.partitioners)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.merge_fused_params">merge_fused_params() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.MergePooledEmbeddingsModule">MergePooledEmbeddingsModule (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.MergePooledEmbeddingsModule">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.DTensorMetadata.mesh">mesh (torchrec.distributed.embedding_types.DTensorMetadata attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy.metadata_info">metadata_info (torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy property)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LFU_EvictionPolicy.metadata_info">(torchrec.modules.mc_modules.LFU_EvictionPolicy property)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LRU_EvictionPolicy.metadata_info">(torchrec.modules.mc_modules.LRU_EvictionPolicy property)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicy.metadata_info">(torchrec.modules.mc_modules.MCHEvictionPolicy property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo.metadata_name">metadata_name (torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup">MetaInferGroupedEmbeddingsLookup (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup">MetaInferGroupedPooledEmbeddingsLookup (class in torchrec.distributed.embedding_lookup)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id9">min_partition (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.min_partition">[1]</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mlp.MLP">MLP (class in torchrec.modules.mlp)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.model_inputs_data">model_inputs_data() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li>
    module

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed">torchrec.distributed</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.collective_utils">torchrec.distributed.collective_utils</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.comm">torchrec.distributed.comm</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.comm_ops">torchrec.distributed.comm_ops</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.dist_data">torchrec.distributed.dist_data</a>, <a href="torchrec.distributed.sharding.html#module-torchrec.distributed.dist_data">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding">torchrec.distributed.embedding</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding_lookup">torchrec.distributed.embedding_lookup</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding_sharding">torchrec.distributed.embedding_sharding</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding_types">torchrec.distributed.embedding_types</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embeddingbag">torchrec.distributed.embeddingbag</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.grouped_position_weighted">torchrec.distributed.grouped_position_weighted</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.mc_embedding">torchrec.distributed.mc_embedding</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.mc_embeddingbag">torchrec.distributed.mc_embeddingbag</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.mc_modules">torchrec.distributed.mc_modules</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.model_parallel">torchrec.distributed.model_parallel</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner">torchrec.distributed.planner</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.constants">torchrec.distributed.planner.constants</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.enumerators">torchrec.distributed.planner.enumerators</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.partitioners">torchrec.distributed.planner.partitioners</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.perf_models">torchrec.distributed.planner.perf_models</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.planners">torchrec.distributed.planner.planners</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.proposers">torchrec.distributed.planner.proposers</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.shard_estimators">torchrec.distributed.planner.shard_estimators</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.stats">torchrec.distributed.planner.stats</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.storage_reservations">torchrec.distributed.planner.storage_reservations</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.types">torchrec.distributed.planner.types</a>
</li>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.utils">torchrec.distributed.planner.utils</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.quant_embeddingbag">torchrec.distributed.quant_embeddingbag</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding">torchrec.distributed.sharding</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.cw_sharding">torchrec.distributed.sharding.cw_sharding</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.dp_sharding">torchrec.distributed.sharding.dp_sharding</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.rw_sharding">torchrec.distributed.sharding.rw_sharding</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.tw_sharding">torchrec.distributed.sharding.tw_sharding</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.twcw_sharding">torchrec.distributed.sharding.twcw_sharding</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.twrw_sharding">torchrec.distributed.sharding.twrw_sharding</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.train_pipeline">torchrec.distributed.train_pipeline</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.types">torchrec.distributed.types</a>
</li>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.utils">torchrec.distributed.utils</a>
</li>
        <li><a href="torchrec.fx.html#module-0">torchrec.fx</a>, <a href="torchrec.fx.html#module-torchrec.fx">[1]</a>
</li>
        <li><a href="torchrec.fx.html#module-torchrec.fx.tracer">torchrec.fx.tracer</a>
</li>
        <li><a href="torchrec.inference.html#module-0">torchrec.inference</a>, <a href="torchrec.inference.html#module-torchrec.inference">[1]</a>
</li>
        <li><a href="torchrec.inference.html#module-torchrec.inference.model_packager">torchrec.inference.model_packager</a>
</li>
        <li><a href="torchrec.inference.html#module-torchrec.inference.modules">torchrec.inference.modules</a>
</li>
        <li><a href="torchrec.models.html#module-torchrec.models.deepfm">torchrec.models.deepfm</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules">torchrec.modules</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.activation">torchrec.modules.activation</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.crossnet">torchrec.modules.crossnet</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.deepfm">torchrec.modules.deepfm</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.embedding_configs">torchrec.modules.embedding_configs</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.embedding_modules">torchrec.modules.embedding_modules</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.feature_processor">torchrec.modules.feature_processor</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.lazy_extension">torchrec.modules.lazy_extension</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.mc_embedding_modules">torchrec.modules.mc_embedding_modules</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.mc_modules">torchrec.modules.mc_modules</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.mlp">torchrec.modules.mlp</a>
</li>
        <li><a href="torchrec.modules.html#module-torchrec.modules.utils">torchrec.modules.utils</a>
</li>
        <li><a href="torchrec.optim.html#module-0">torchrec.optim</a>, <a href="torchrec.optim.html#module-torchrec.optim">[1]</a>
</li>
        <li><a href="torchrec.optim.html#module-torchrec.optim.clipping">torchrec.optim.clipping</a>
</li>
        <li><a href="torchrec.optim.html#module-torchrec.optim.fused">torchrec.optim.fused</a>
</li>
        <li><a href="torchrec.optim.html#module-torchrec.optim.keyed">torchrec.optim.keyed</a>
</li>
        <li><a href="torchrec.optim.html#module-torchrec.optim.warmup">torchrec.optim.warmup</a>
</li>
        <li><a href="torchrec.quant.html#module-0">torchrec.quant</a>, <a href="torchrec.quant.html#module-torchrec.quant">[1]</a>
</li>
        <li><a href="torchrec.quant.html#module-torchrec.quant.embedding_modules">torchrec.quant.embedding_modules</a>
</li>
        <li><a href="torchrec.sparse.html#module-0">torchrec.sparse</a>, <a href="torchrec.sparse.html#module-torchrec.sparse">[1]</a>
</li>
        <li><a href="torchrec.sparse.html#module-torchrec.sparse.jagged_tensor">torchrec.sparse.jagged_tensor</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.module">module (torchrec.distributed.model_parallel.DistributedModelParallel property)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.module">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#id16">(torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.module_pooled">module_pooled() (torchrec.distributed.planner.types.ShardingOption static method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionSharder.module_type">module_type (torchrec.distributed.embedding.EmbeddingCollectionSharder property)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder.module_type">(torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagSharder.module_type">(torchrec.distributed.embeddingbag.EmbeddingBagSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionSharder.module_type">(torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionSharder.module_type">(torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder.module_type">(torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantEmbeddingBagCollectionSharder.module_type">(torchrec.distributed.quant_embeddingbag.QuantEmbeddingBagCollectionSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder.module_type">(torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder.module_type">(torchrec.distributed.types.ModuleSharder property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder">ModuleSharder (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ModuleShardingMixIn">ModuleShardingMixIn (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleShardingPlan">ModuleShardingPlan (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams.multipass_prefetch_config">multipass_prefetch_config (torchrec.distributed.types.CacheParams attribute)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="N">N</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.name">name (torchrec.distributed.planner.types.ShardingOption attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.name">(torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.named_buffers">named_buffers() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.named_buffers">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin.named_buffers">(torchrec.distributed.embedding_lookup.InferGroupedLookupMixin method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.named_buffers">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.named_buffers">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.named_buffers">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule.named_buffers">(torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.named_buffers">(torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedProcessor.named_buffers">(torchrec.modules.feature_processor.PositionWeightedProcessor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.named_modules">named_modules() (torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.named_parameters">named_parameters() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.named_parameters">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin.named_parameters">(torchrec.distributed.embedding_lookup.InferGroupedLookupMixin method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.named_parameters">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.named_parameters">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.named_parameters">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule.named_parameters">(torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.named_parameters">(torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.named_parameters_by_table">named_parameters_by_table() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.named_parameters_by_table">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.need_indices">need_indices() (torchrec.modules.embedding_modules.EmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollectionInterface.need_indices">(torchrec.modules.embedding_modules.EmbeddingCollectionInterface method)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.need_indices">(torchrec.quant.embedding_modules.EmbeddingCollection method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.need_pos">need_pos (torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.QualNameMetadata.need_preproc">need_preproc (torchrec.inference.modules.QualNameMetadata attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.BinarySearchPredicate.next">next() (torchrec.distributed.planner.utils.BinarySearchPredicate method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.LuusJaakolaSearch.next">(torchrec.distributed.planner.utils.LuusJaakolaSearch method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.next_plan">next_plan() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.PipelineType.NONE">NONE (torchrec.distributed.types.PipelineType attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.PoolingType.NONE">(torchrec.modules.embedding_configs.PoolingType attribute)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.clipping.GradientClipping.NONE">(torchrec.optim.clipping.GradientClipping attribute)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy.NONE">(torchrec.optim.warmup.WarmupPolicy attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.none_throws">none_throws() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.stats.NoopEmbeddingStats">NoopEmbeddingStats (class in torchrec.distributed.planner.stats)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.perf_models.NoopPerfModel">NoopPerfModel (class in torchrec.distributed.planner.perf_models)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NoOpQuantizedCommCodec">NoOpQuantizedCommCodec (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.perf_models.NoopStorageModel">NoopStorageModel (class in torchrec.distributed.planner.perf_models)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.clipping.GradientClipping.NORM">NORM (torchrec.optim.clipping.GradientClipping attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NoWait">NoWait (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NullShardedModuleContext">NullShardedModuleContext (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NullShardingContext">NullShardingContext (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.num_embeddings">num_embeddings (torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingConfig.num_embeddings">(torchrec.modules.embedding_configs.EmbeddingConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.num_features">num_features() (torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.num_features">(torchrec.modules.embedding_configs.BaseEmbeddingConfig method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.num_inputs">num_inputs (torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id10">num_poolings (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.num_poolings">[1]</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.num_shards">num_shards (torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="O">O</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ObjectPoolShardingPlan">ObjectPoolShardingPlan (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ObjectPoolShardingType">ObjectPoolShardingType (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Shard.offset">offset (torchrec.distributed.planner.types.Shard attribute)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.offset_per_key">offset_per_key() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.offset_per_key">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.offset_per_key_or_none">offset_per_key_or_none() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.offsets">offsets() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.offsets">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.offsets_or_none">offsets_or_none() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.offsets_or_none">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.offsets_to_range_traceble">offsets_to_range_traceble() (in module torchrec.modules.feature_processor)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.optimizer_type_to_emb_opt_type">optimizer_type_to_emb_opt_type() (in module torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.optimizers">optimizers (torchrec.optim.keyed.CombinedOptimizer property)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper">OptimizerWrapper (class in torchrec.optim.keyed)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType">OptimType (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.OrderedDeviceHardware">OrderedDeviceHardware (class in torchrec.distributed.planner.partitioners)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PlannerErrorType.OTHER">OTHER (torchrec.distributed.planner.types.PlannerErrorType attribute)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.output_dist">output_dist() (torchrec.distributed.embedding.ShardedEmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.output_dist">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.output_dist">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection.output_dist">(torchrec.distributed.mc_modules.ShardedManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.output_dist">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.output_dist">(torchrec.distributed.types.ShardedModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#id11">output_dtype (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.output_dtype">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.output_dtype">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.output_dtype">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection.output_dtype">output_dtype() (torchrec.quant.embedding_modules.EmbeddingBagCollection method)</a>

      <ul>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.output_dtype">(torchrec.quant.embedding_modules.EmbeddingCollection method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.output_size">output_size() (torchrec.modules.mc_modules.ManagedCollisionModule method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.output_size">(torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id19">output_split_sizes (torchrec.distributed.comm_ops.All2AllVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllVInfo.output_split_sizes">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllDenseInfo.output_splits">output_splits (torchrec.distributed.comm_ops.All2AllDenseInfo attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.html#id11">(torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.output_splits">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#id33">(torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo.output_splits">[1]</a>
</li>
      </ul></li>
      <li><a href="torchrec.models.html#torchrec.models.deepfm.OverArch">OverArch (class in torchrec.models.deepfm)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="P">P</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.pad_vbe_kjt_lengths">pad_vbe_kjt_lengths() (in module torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo.param">param (torchrec.distributed.embedding_sharding.EmbeddingShardingInfo attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.ShardingOptionGroup.param_count">param_count (torchrec.distributed.planner.partitioners.ShardingOptionGroup attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.param_groups">param_groups (torchrec.optim.keyed.CombinedOptimizer property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo.param_sharding">param_sharding (torchrec.distributed.embedding_sharding.EmbeddingShardingInfo attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints">ParameterConstraints (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding">ParameterSharding (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterStorage">ParameterStorage (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.params">params (torchrec.optim.keyed.CombinedOptimizer property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.PARTIAL_ROWWISE_ADAM">PARTIAL_ROWWISE_ADAM (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.PARTIAL_ROWWISE_LAMB">PARTIAL_ROWWISE_LAMB (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PlannerErrorType.PARTITION">PARTITION (torchrec.distributed.planner.types.PlannerErrorType attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.GreedyPerfPartitioner.partition">partition() (torchrec.distributed.planner.partitioners.GreedyPerfPartitioner method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.MemoryBalancedPartitioner.partition">(torchrec.distributed.planner.partitioners.MemoryBalancedPartitioner method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Partitioner.partition">(torchrec.distributed.planner.types.Partitioner method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PartitionByType">PartitionByType (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Partitioner">Partitioner (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.path">path (torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      <li><a href="torchrec.fx.html#torchrec.fx.tracer.Tracer.path_of_module">path_of_module() (torchrec.fx.tracer.Tracer method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.QuantConfig.per_table_weight_dtype">per_table_weight_dtype (torchrec.modules.embedding_configs.QuantConfig attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mlp.Perceptron">Perceptron (class in torchrec.modules.mlp)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Perf">Perf (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.SortBy.PERF">PERF (torchrec.distributed.planner.partitioners.SortBy attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.DeviceHardware.perf">perf (torchrec.distributed.planner.types.DeviceHardware attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Shard.perf">(torchrec.distributed.planner.types.Shard attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.perf_func_emb_wall_time">perf_func_emb_wall_time() (torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator class method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.ShardingOptionGroup.perf_sum">perf_sum (torchrec.distributed.planner.partitioners.ShardingOptionGroup attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PerfModel">PerfModel (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.permute">permute() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.html#id12">permuted_lengths_after_sparse_data_all2all (torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.permuted_lengths_after_sparse_data_all2all">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.pg">pg (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.pg_name">pg_name() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.pin_memory">pin_memory() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.BatchingMetadata.pinned">pinned (torchrec.inference.modules.BatchingMetadata attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.PipelineType">PipelineType (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.placement">placement() (in module torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.DTensorMetadata.placements">placements (torchrec.distributed.embedding_types.DTensorMetadata attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.plan">plan (torchrec.distributed.model_parallel.DistributedModelParallel property)</a>

      <ul>
        <li><a href="torchrec.distributed.html#id42">(torchrec.distributed.types.ShardingPlan attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.ShardingPlan.plan">[1]</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.planners.EmbeddingShardingPlanner.plan">plan() (torchrec.distributed.planner.planners.EmbeddingShardingPlanner method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.planners.HeteroEmbeddingShardingPlanner.plan">(torchrec.distributed.planner.planners.HeteroEmbeddingShardingPlanner method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingPlanner.plan">(torchrec.distributed.types.ShardingPlanner method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PlannerError">PlannerError</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PlannerErrorType">PlannerErrorType (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupStage.policy">policy (torchrec.optim.warmup.WarmupStage attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy.POLY">POLY (torchrec.optim.warmup.WarmupPolicy attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL">POOLED_EMBEDDINGS_ALL_TO_ALL (torchrec.distributed.types.CommOp attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER">POOLED_EMBEDDINGS_REDUCE_SCATTER (torchrec.distributed.types.CommOp attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAllGather">PooledEmbeddingsAllGather (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAllGather">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll">PooledEmbeddingsAllToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll">[1]</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable">PooledEmbeddingsAwaitable (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter">PooledEmbeddingsReduceScatter (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.pooling">pooling (torchrec.distributed.embedding_types.GroupedEmbeddingConfig attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingBagConfig.pooling">(torchrec.modules.embedding_configs.EmbeddingBagConfig attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig.pooling">(torchrec.modules.embedding_configs.EmbeddingTableConfig attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#id12">pooling_factors (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.pooling_factors">[1]</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.pooling_type_to_pooling_mode">pooling_type_to_pooling_mode() (in module torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.pooling_type_to_str">pooling_type_to_str() (in module torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.PoolingType">PoolingType (class in torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.enumerators.EmbeddingEnumerator.populate_estimates">populate_estimates() (torchrec.distributed.planner.enumerators.EmbeddingEnumerator method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Enumerator.populate_estimates">(torchrec.distributed.planner.types.Enumerator method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.position_weighted_module_update_features">position_weighted_module_update_features() (in module torchrec.modules.feature_processor)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedModule">PositionWeightedModule (class in torchrec.modules.feature_processor)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedProcessor">PositionWeightedProcessor (class in torchrec.modules.feature_processor)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.post_load_state_dict">post_load_state_dict() (torchrec.optim.keyed.CombinedOptimizer method)</a>

      <ul>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizer.post_load_state_dict">(torchrec.optim.keyed.KeyedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper.post_load_state_dict">(torchrec.optim.keyed.OptimizerWrapper method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupOptimizer.post_load_state_dict">(torchrec.optim.warmup.WarmupOptimizer method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id36">precision (torchrec.distributed.types.CacheParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams.precision">[1]</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictModule.predict_forward">predict_forward() (torchrec.inference.modules.PredictModule method)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictModule.predict_module">predict_module (torchrec.inference.modules.PredictModule property)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory">PredictFactory (class in torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.model_packager.PredictFactoryPackager">PredictFactoryPackager (class in torchrec.inference.model_packager)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictModule">PredictModule (class in torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.prefetch">prefetch() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.prefetch">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedEmbeddingModule.prefetch">(torchrec.distributed.embedding_types.ShardedEmbeddingModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Perf.prefetch_compute">prefetch_compute (torchrec.distributed.planner.types.Perf attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#id37">prefetch_pipeline (torchrec.distributed.types.CacheParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams.prefetch_pipeline">[1]</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.prepend_opt_key">prepend_opt_key() (torchrec.optim.keyed.CombinedOptimizer static method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.preprocess">preprocess() (torchrec.modules.mc_modules.ManagedCollisionModule method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.preprocess">(torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.probabilistic_threshold_filter">probabilistic_threshold_filter() (in module torchrec.modules.mc_modules)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.process_pooled_embeddings">process_pooled_embeddings() (in module torchrec.modules.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.prod">prod() (in module torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.profile">profile() (torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.promote_high_prefetch_overheaad_table_to_hbm">promote_high_prefetch_overheaad_table_to_hbm() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer static method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer.propose">propose() (torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GreedyProposer.propose">(torchrec.distributed.planner.proposers.GreedyProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.GridSearchProposer.propose">(torchrec.distributed.planner.proposers.GridSearchProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.UniformProposer.propose">(torchrec.distributed.planner.proposers.UniformProposer method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Proposer.propose">(torchrec.distributed.planner.types.Proposer method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Proposer">Proposer (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.proposers_to_proposals_list">proposers_to_proposals_list() (in module torchrec.distributed.planner.proposers)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.pruned_num_embeddings">pruned_num_embeddings() (in module torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.pruning_indices_remapping">pruning_indices_remapping (torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#id40">ps_hosts (torchrec.distributed.types.KeyValueParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.KeyValueParams.ps_hosts">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.purge">purge() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.purge">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.purge">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.purge">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="Q">Q</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.qcomm_codecs_registry">qcomm_codecs_registry (torchrec.distributed.embedding_sharding.EmbeddingSharding property)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder.qcomm_codecs_registry">(torchrec.distributed.types.ModuleSharder property)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.qcomm_codecs_registry">(torchrec.distributed.types.ShardedModule property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.qualname_metadata">qualname_metadata() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.qualname_metadata_json">qualname_metadata_json() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.QualNameMetadata">QualNameMetadata (class in torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.QUANT">QUANT (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.quant_prep_customize_row_alignment">quant_prep_customize_row_alignment() (in module torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.quant_prep_enable_quant_state_dict_split_scale_bias">quant_prep_enable_quant_state_dict_split_scale_bias() (in module torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.quant_prep_enable_quant_state_dict_split_scale_bias_for_types">quant_prep_enable_quant_state_dict_split_scale_bias_for_types() (in module torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.quant_prep_enable_register_tbes">quant_prep_enable_register_tbes() (in module torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.QUANT_UVM">QUANT_UVM (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.EmbeddingComputeKernel.QUANT_UVM_CACHING">QUANT_UVM_CACHING (torchrec.distributed.embedding_types.EmbeddingComputeKernel attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.QuantConfig">QuantConfig (class in torchrec.modules.embedding_configs)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantEmbeddingBagCollectionSharder">QuantEmbeddingBagCollectionSharder (class in torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder">QuantFeatureProcessedEmbeddingBagCollectionSharder (class in torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.quantize_dense">quantize_dense() (in module torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.quantize_embeddings">quantize_embeddings() (in module torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.quantize_feature">quantize_feature() (in module torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.quantize_inference_model">quantize_inference_model() (in module torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.quantize_state_dict">quantize_state_dict() (in module torchrec.quant.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodec.quantized_dtype">quantized_dtype (torchrec.distributed.types.QuantizedCommCodec property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.NoOpQuantizedCommCodec.quantized_dtype">quantized_dtype() (torchrec.distributed.types.NoOpQuantizedCommCodec method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodec">QuantizedCommCodec (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs">QuantizedCommCodecs (class in torchrec.distributed.types)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="R">R</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.DeviceHardware.rank">rank (torchrec.distributed.planner.types.DeviceHardware attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Shard.rank">(torchrec.distributed.planner.types.Shard attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.rank_device">rank_device() (in module torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.ranks">ranks (torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.perf_models.NoopPerfModel.rate">rate() (torchrec.distributed.planner.perf_models.NoopPerfModel method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.perf_models.NoopStorageModel.rate">(torchrec.distributed.planner.perf_models.NoopStorageModel method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PerfModel.rate">(torchrec.distributed.planner.types.PerfModel method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.rebuild_with_output_id_range">rebuild_with_output_id_range() (torchrec.modules.mc_modules.ManagedCollisionModule method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.rebuild_with_output_id_range">(torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.SequenceVBEContext.recat">recat (torchrec.modules.utils.SequenceVBEContext attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy.record_history_metadata">record_history_metadata() (torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LFU_EvictionPolicy.record_history_metadata">(torchrec.modules.mc_modules.LFU_EvictionPolicy method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LRU_EvictionPolicy.record_history_metadata">(torchrec.modules.mc_modules.LRU_EvictionPolicy method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicy.record_history_metadata">(torchrec.modules.mc_modules.MCHEvictionPolicy method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionContext.record_stream">record_stream() (torchrec.distributed.embedding.EmbeddingCollectionContext method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext.record_stream">(torchrec.distributed.embedding_sharding.EmbeddingShardingContext method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.InputDistOutputs.record_stream">(torchrec.distributed.embedding_types.InputDistOutputs method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList.record_stream">(torchrec.distributed.embedding_types.KJTList method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ListOfKJTList.record_stream">(torchrec.distributed.embedding_types.ListOfKJTList method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext.record_stream">(torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext.record_stream">(torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext.record_stream">(torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.NullShardedModuleContext.record_stream">(torchrec.distributed.types.NullShardedModuleContext method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.NullShardingContext.record_stream">(torchrec.distributed.types.NullShardingContext method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.utils.SequenceVBEContext.record_stream">(torchrec.modules.utils.SequenceVBEContext method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.record_stream">(torchrec.sparse.jagged_tensor.JaggedTensor method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.record_stream">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.record_stream">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_base_pooled">reduce_scatter_base_pooled() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_base_sync">reduce_scatter_base_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_pooled">reduce_scatter_pooled() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_sync">reduce_scatter_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_tensor_backward">reduce_scatter_tensor_backward() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_tensor_fake">reduce_scatter_tensor_fake() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_tensor_setup_context">reduce_scatter_tensor_setup_context() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_v_per_feature_pooled">reduce_scatter_v_per_feature_pooled() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_v_pooled">reduce_scatter_v_pooled() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.reduce_scatter_v_sync">reduce_scatter_v_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatter_Req">ReduceScatter_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatter_Wait">ReduceScatter_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBase_Req">ReduceScatterBase_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBase_Wait">ReduceScatterBase_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterBaseInfo">ReduceScatterBaseInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterInfo">ReduceScatterInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterV_Req">ReduceScatterV_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterV_Wait">ReduceScatterV_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterVInfo">ReduceScatterVInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.regroup">regroup() (torchrec.sparse.jagged_tensor.KeyedTensor static method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.regroup_as_dict">regroup_as_dict() (torchrec.sparse.jagged_tensor.KeyedTensor static method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.SequenceVBEContext.reindexed_length_per_key">reindexed_length_per_key (torchrec.modules.utils.SequenceVBEContext attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.SequenceVBEContext.reindexed_lengths">reindexed_lengths (torchrec.modules.utils.SequenceVBEContext attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.SequenceVBEContext.reindexed_values">reindexed_values (torchrec.modules.utils.SequenceVBEContext attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.remap">remap() (torchrec.modules.mc_modules.MCHManagedCollisionModule method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext.remapped_kjt">remapped_kjt (torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.reorder_inverse_indices">reorder_inverse_indices() (in module torchrec.modules.embedding_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.replace_placement_with_meta_device">replace_placement_with_meta_device() (in module torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ObjectPoolShardingType.REPLICATED_ROW_WISE">REPLICATED_ROW_WISE (torchrec.distributed.types.ObjectPoolShardingType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.Request">Request (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.FixedPercentageStorageReservation.reserve">reserve() (torchrec.distributed.planner.storage_reservations.FixedPercentageStorageReservation method)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation.reserve">(torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.InferenceStorageReservation.reserve">(torchrec.distributed.planner.storage_reservations.InferenceStorageReservation method)</a>
</li>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.StorageReservation.reserve">(torchrec.distributed.planner.types.StorageReservation method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#id38">reserved_memory (torchrec.distributed.types.CacheParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams.reserved_memory">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.reset_parameters">reset_parameters() (torchrec.distributed.embedding.ShardedEmbeddingCollection method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.reset_parameters">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection.reset_parameters">(torchrec.modules.embedding_modules.EmbeddingBagCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.reset_parameters">(torchrec.modules.embedding_modules.EmbeddingCollection method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedModule.reset_parameters">(torchrec.modules.feature_processor.PositionWeightedModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.reset_shard_rank">reset_shard_rank() (in module torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.result_metadata">result_metadata() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.stats.round_to_one_sigfig">round_to_one_sigfig() (in module torchrec.distributed.planner.stats)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ObjectPoolShardingType.ROW_WISE">ROW_WISE (torchrec.distributed.types.ObjectPoolShardingType attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingType.ROW_WISE">(torchrec.distributed.types.ShardingType attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.ShardingType.ROW_WISE">(torchrec.modules.embedding_configs.ShardingType attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.ROWWISE_ADAGRAD">ROWWISE_ADAGRAD (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.collective_utils.run_on_leader">run_on_leader() (in module torchrec.distributed.collective_utils)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.run_weights_dependent_transformations">run_weights_dependent_transformations() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictFactory.run_weights_independent_tranformations">run_weights_independent_tranformations() (torchrec.inference.modules.PredictFactory method)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist">RwPooledEmbeddingDist (class in torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding">RwPooledEmbeddingSharding (class in torchrec.distributed.sharding.rw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist">RwSparseFeaturesDist (class in torchrec.distributed.sharding.rw_sharding)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="S">S</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.save_param_groups">save_param_groups() (torchrec.optim.keyed.CombinedOptimizer method)</a>

      <ul>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizer.save_param_groups">(torchrec.optim.keyed.KeyedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper.save_param_groups">(torchrec.optim.keyed.OptimizerWrapper method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.inference.html#torchrec.inference.model_packager.PredictFactoryPackager.save_predict_factory">save_predict_factory() (torchrec.inference.model_packager.PredictFactoryPackager class method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.scope">scope() (in module torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne">SeqEmbeddingsAllToOne (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL">SEQUENCE_EMBEDDINGS_ALL_TO_ALL (torchrec.distributed.types.CommOp attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll">SequenceEmbeddingsAllToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable">SequenceEmbeddingsAwaitable (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable">[1]</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.SequenceVBEContext">SequenceVBEContext (class in torchrec.modules.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOne.set_device">set_device() (torchrec.distributed.dist_data.EmbeddingsAllToOne method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOne.set_device">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce.set_device">(torchrec.distributed.dist_data.EmbeddingsAllToOneReduce method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce.set_device">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.set_device">(torchrec.distributed.dist_data.SeqEmbeddingsAllToOne method)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.set_device">[1]</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.set_ec_index_dedup">set_ec_index_dedup() (in module torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.model_packager.PredictFactoryPackager.set_extern_modules">set_extern_modules() (torchrec.inference.model_packager.PredictFactoryPackager class method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.set_gradient_division">set_gradient_division() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.set_hbm_per_device">set_hbm_per_device() (in module torchrec.distributed.planner.partitioners)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.model_packager.PredictFactoryPackager.set_mocked_modules">set_mocked_modules() (torchrec.inference.model_packager.PredictFactoryPackager class method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.set_use_sync_collectives">set_use_sync_collectives() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.SGD">SGD (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupStage.sgdr_period">sgdr_period (torchrec.optim.warmup.WarmupStage attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.SHAMPOO">SHAMPOO (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.OptimType.SHAMPOO_V2">SHAMPOO_V2 (torchrec.distributed.embedding_types.OptimType attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Shard">Shard (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionSharder.shard">shard() (torchrec.distributed.embedding.EmbeddingCollectionSharder method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder.shard">(torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagSharder.shard">(torchrec.distributed.embeddingbag.EmbeddingBagSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionSharder.shard">(torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionSharder.shard">(torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder.shard">(torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantEmbeddingBagCollectionSharder.shard">(torchrec.distributed.quant_embeddingbag.QuantEmbeddingBagCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder.shard">(torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder.shard">(torchrec.distributed.types.ModuleSharder class method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.shard_quant_model">shard_quant_model() (in module torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionSharder.shardable_parameters">shardable_parameters() (torchrec.distributed.embedding.EmbeddingCollectionSharder method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder.shardable_parameters">(torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder.shardable_parameters">(torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagSharder.shardable_parameters">(torchrec.distributed.embeddingbag.EmbeddingBagSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder.shardable_parameters">(torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder.shardable_parameters">(torchrec.distributed.types.ModuleSharder method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.utils.sharded_model_copy">sharded_model_copy (class in torchrec.distributed.utils)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.sharded_parameter_names">sharded_parameter_names() (torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection.sharded_parameter_names">(torchrec.distributed.mc_modules.ShardedManagedCollisionCollection method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.sharded_parameter_names">(torchrec.distributed.types.ShardedModule method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedConfig">ShardedConfig (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag">ShardedEmbeddingBag (class in torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection">ShardedEmbeddingBagCollection (class in torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection">ShardedEmbeddingCollection (class in torchrec.distributed.embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedEmbeddingModule">ShardedEmbeddingModule (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedEmbeddingTable">ShardedEmbeddingTable (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection">ShardedManagedCollisionCollection (class in torchrec.distributed.mc_modules)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ShardedManagedCollisionEmbeddingBagCollection">ShardedManagedCollisionEmbeddingBagCollection (class in torchrec.distributed.mc_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ShardedManagedCollisionEmbeddingCollection">ShardedManagedCollisionEmbeddingCollection (class in torchrec.distributed.mc_embedding)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedMetaConfig">ShardedMetaConfig (class in torchrec.distributed.embedding_types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule">ShardedModule (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEbcInputDist">ShardedQuantEbcInputDist (class in torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection">ShardedQuantEmbeddingBagCollection (class in torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection">ShardedQuantFeatureProcessedEmbeddingBagCollection (class in torchrec.distributed.quant_embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.sharder_name">sharder_name() (in module torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardEstimator">ShardEstimator (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext.sharding_contexts">sharding_contexts (torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.ShardingOptionGroup.sharding_options">sharding_options (torchrec.distributed.planner.partitioners.ShardingOptionGroup attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.sharding_spec">sharding_spec (torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.sharding_type">sharding_type (torchrec.distributed.planner.types.ShardingOption attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ObjectPoolShardingPlan.sharding_type">(torchrec.distributed.types.ObjectPoolShardingPlan attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.sharding_type">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.sharding_type_device_group_to_sharding_infos">sharding_type_device_group_to_sharding_infos() (torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#id13">sharding_types (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.sharding_types">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.EmbeddingCollectionSharder.sharding_types">sharding_types() (torchrec.distributed.embedding.EmbeddingCollectionSharder method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingSharder.sharding_types">(torchrec.distributed.embedding_types.BaseEmbeddingSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder.sharding_types">(torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder.sharding_types">(torchrec.distributed.mc_modules.ManagedCollisionCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder.sharding_types">(torchrec.distributed.quant_embeddingbag.QuantFeatureProcessedEmbeddingBagCollectionSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder.sharding_types">(torchrec.distributed.types.ModuleSharder method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv">ShardingEnv (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption">ShardingOption (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.ShardingOptionGroup">ShardingOptionGroup (class in torchrec.distributed.planner.partitioners)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingPlan">ShardingPlan (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingPlanner">ShardingPlanner (class in torchrec.distributed.types)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ModuleShardingMixIn.shardings">shardings (torchrec.distributed.embedding_types.ModuleShardingMixIn property)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.shardings">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingType">ShardingType (class in torchrec.distributed.types)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.ShardingType">(class in torchrec.modules.embedding_configs)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.shards">shards (torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.LuusJaakolaSearch.shrink_right">shrink_right() (torchrec.distributed.planner.utils.LuusJaakolaSearch method)</a>
</li>
      <li><a href="torchrec.models.html#torchrec.models.deepfm.SimpleDeepFMNN">SimpleDeepFMNN (class in torchrec.models.deepfm)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.DTensorMetadata.size">size (torchrec.distributed.embedding_types.DTensorMetadata attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Shard.size">(torchrec.distributed.planner.types.Shard attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.SortBy">SortBy (class in torchrec.distributed.planner.partitioners)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.sparse_grad_parameter_names">sparse_grad_parameter_names() (torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
      <li><a href="torchrec.models.html#torchrec.models.deepfm.SparseArch">SparseArch (class in torchrec.models.deepfm)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.split">split() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.splits">splits (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.splits_tensors">splits_tensors (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SplitsAllToAllAwaitable">SplitsAllToAllAwaitable (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SplitsAllToAllAwaitable">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#id41">ssd_storage_directory (torchrec.distributed.types.KeyValueParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.KeyValueParams.ssd_storage_directory">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta.stagger">stagger (torchrec.distributed.embedding_sharding.KJTSplitsAllToAllMeta attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.state">state (torchrec.optim.keyed.CombinedOptimizer property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict">state_dict() (torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin.state_dict">(torchrec.distributed.embedding_lookup.InferGroupedLookupMixin method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule.state_dict">(torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict">(torchrec.distributed.model_parallel.DistributedModelParallel method)</a>
</li>
        <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictModule.state_dict">(torchrec.inference.modules.PredictModule method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedProcessor.state_dict">(torchrec.modules.feature_processor.PositionWeightedProcessor method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizer.state_dict">(torchrec.optim.keyed.KeyedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper.state_dict">(torchrec.optim.keyed.OptimizerWrapper method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Stats">Stats (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.html#id39">stats (torchrec.distributed.types.CacheParams attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.types.CacheParams.stats">[1]</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy.STEP">STEP (torchrec.optim.warmup.WarmupPolicy attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.clipping.GradientClippingOptimizer.step">step() (torchrec.optim.clipping.GradientClippingOptimizer method)</a>

      <ul>
        <li><a href="torchrec.optim.html#torchrec.optim.fused.EmptyFusedOptimizer.step">(torchrec.optim.fused.EmptyFusedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.fused.FusedOptimizer.step">(torchrec.optim.fused.FusedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.step">(torchrec.optim.keyed.CombinedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizerWrapper.step">(torchrec.optim.keyed.KeyedOptimizerWrapper method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper.step">(torchrec.optim.keyed.OptimizerWrapper method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupOptimizer.step">(torchrec.optim.warmup.WarmupOptimizer method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#id14">stochastic_rounding (torchrec.distributed.planner.types.ParameterConstraints attribute)</a>, <a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ParameterConstraints.stochastic_rounding">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.stochastic_rounding">(torchrec.distributed.planner.types.ShardingOption attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ParameterSharding.stochastic_rounding">(torchrec.distributed.types.ParameterSharding attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Storage">Storage (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.SortBy.STORAGE">STORAGE (torchrec.distributed.planner.partitioners.SortBy attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.DeviceHardware.storage">storage (torchrec.distributed.planner.types.DeviceHardware attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Shard.storage">(torchrec.distributed.planner.types.Shard attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.storage_repr_in_gb">storage_repr_in_gb() (in module torchrec.distributed.planner.utils)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.partitioners.ShardingOptionGroup.storage_sum">storage_sum (torchrec.distributed.planner.partitioners.ShardingOptionGroup attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingSharder.storage_usage">storage_usage() (torchrec.distributed.embedding_types.BaseEmbeddingSharder method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder.storage_usage">(torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder method)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ModuleSharder.storage_usage">(torchrec.distributed.types.ModuleSharder method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.StorageReservation">StorageReservation (class in torchrec.distributed.planner.types)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PlannerErrorType.STRICT_CONSTRAINTS">STRICT_CONSTRAINTS (torchrec.distributed.planner.types.PlannerErrorType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.DTensorMetadata.stride">stride (torchrec.distributed.embedding_types.DTensorMetadata attribute)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.stride">stride() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.stride_per_key">stride_per_key() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.stride_per_key_per_rank">stride_per_key_per_rank() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.PoolingType.SUM">SUM (torchrec.modules.embedding_configs.PoolingType attribute)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.CustomTopologyData.supported_fields">supported_fields (torchrec.distributed.planner.types.CustomTopologyData attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.activation.SwishLayerNorm">SwishLayerNorm (class in torchrec.modules.activation)</a>
</li>
      <li><a href="torchrec.fx.html#torchrec.fx.tracer.symbolic_trace">symbolic_trace() (in module torchrec.fx.tracer)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.sync">sync() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="T">T</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingType.TABLE_COLUMN_WISE">TABLE_COLUMN_WISE (torchrec.distributed.types.ShardingType attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.ShardingType.TABLE_COLUMN_WISE">(torchrec.modules.embedding_configs.ShardingType attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.GroupedEmbeddingConfig.table_names">table_names() (torchrec.distributed.embedding_types.GroupedEmbeddingConfig method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingType.TABLE_ROW_WISE">TABLE_ROW_WISE (torchrec.distributed.types.ShardingType attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.ShardingType.TABLE_ROW_WISE">(torchrec.modules.embedding_configs.ShardingType attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardingType.TABLE_WISE">TABLE_WISE (torchrec.distributed.types.ShardingType attribute)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.ShardingType.TABLE_WISE">(torchrec.modules.embedding_configs.ShardingType attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection.tbes">tbes (torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection attribute)</a>

      <ul>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection.tbes">(torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.tbes_configs">tbes_configs() (torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.tensor">tensor (torchrec.distributed.planner.types.ShardingOption attribute)</a>

      <ul>
        <li><a href="torchrec.distributed.planner.html#id17">(torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorAllToAll">TensorAllToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorAllToAll">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorAllToAllSplitsAwaitable">TensorAllToAllSplitsAwaitable (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorAllToAllSplitsAwaitable">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorAllToAllValuesAwaitable">TensorAllToAllValuesAwaitable (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorAllToAllValuesAwaitable">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorValuesAllToAll">TensorValuesAllToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorValuesAllToAll">[1]</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.to">to() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.to">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.to">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.to_dense">to_dense() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.to_dense_weights">to_dense_weights() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.to_dict">to_dict() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.to_dict">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.to_padded_dense">to_padded_dense() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.to_padded_dense_weights">to_padded_dense_weights() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology">Topology (class in torchrec.distributed.planner.types)</a>
</li>
      <li>
    torchrec.distributed

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.collective_utils

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.collective_utils">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.comm

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.comm">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.comm_ops

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.comm_ops">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.dist_data

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.dist_data">module</a>, <a href="torchrec.distributed.sharding.html#module-torchrec.distributed.dist_data">[1]</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.embedding

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.embedding_lookup

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding_lookup">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.embedding_sharding

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding_sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.embedding_types

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embedding_types">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.embeddingbag

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.embeddingbag">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.grouped_position_weighted

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.grouped_position_weighted">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.mc_embedding

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.mc_embedding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.mc_embeddingbag

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.mc_embeddingbag">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.mc_modules

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.mc_modules">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.model_parallel

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.model_parallel">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.constants

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.constants">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.enumerators

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.enumerators">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.partitioners

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.partitioners">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.perf_models

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.perf_models">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.planners

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.planners">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.proposers

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.proposers">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.shard_estimators

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.shard_estimators">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.stats

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.stats">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.storage_reservations

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.storage_reservations">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.types

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.types">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.planner.utils

      <ul>
        <li><a href="torchrec.distributed.planner.html#module-torchrec.distributed.planner.utils">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.quant_embeddingbag

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.quant_embeddingbag">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.sharding

      <ul>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.sharding.cw_sharding

      <ul>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.cw_sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.sharding.dp_sharding

      <ul>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.dp_sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.sharding.rw_sharding

      <ul>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.rw_sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.sharding.tw_sharding

      <ul>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.tw_sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.sharding.twcw_sharding

      <ul>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.twcw_sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.sharding.twrw_sharding

      <ul>
        <li><a href="torchrec.distributed.sharding.html#module-torchrec.distributed.sharding.twrw_sharding">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.train_pipeline

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.train_pipeline">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.types

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.types">module</a>
</li>
      </ul></li>
      <li>
    torchrec.distributed.utils

      <ul>
        <li><a href="torchrec.distributed.html#module-torchrec.distributed.utils">module</a>
</li>
      </ul></li>
      <li>
    torchrec.fx

      <ul>
        <li><a href="torchrec.fx.html#module-0">module</a>, <a href="torchrec.fx.html#module-torchrec.fx">[1]</a>
</li>
      </ul></li>
      <li>
    torchrec.fx.tracer

      <ul>
        <li><a href="torchrec.fx.html#module-torchrec.fx.tracer">module</a>
</li>
      </ul></li>
      <li>
    torchrec.inference

      <ul>
        <li><a href="torchrec.inference.html#module-0">module</a>, <a href="torchrec.inference.html#module-torchrec.inference">[1]</a>
</li>
      </ul></li>
      <li>
    torchrec.inference.model_packager

      <ul>
        <li><a href="torchrec.inference.html#module-torchrec.inference.model_packager">module</a>
</li>
      </ul></li>
      <li>
    torchrec.inference.modules

      <ul>
        <li><a href="torchrec.inference.html#module-torchrec.inference.modules">module</a>
</li>
      </ul></li>
      <li>
    torchrec.models.deepfm

      <ul>
        <li><a href="torchrec.models.html#module-torchrec.models.deepfm">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.activation

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.activation">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.crossnet

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.crossnet">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.deepfm

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.deepfm">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.embedding_configs

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.embedding_configs">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.embedding_modules

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.embedding_modules">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.feature_processor

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.feature_processor">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.lazy_extension

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.lazy_extension">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.mc_embedding_modules

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.mc_embedding_modules">module</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li>
    torchrec.modules.mc_modules

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.mc_modules">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.mlp

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.mlp">module</a>
</li>
      </ul></li>
      <li>
    torchrec.modules.utils

      <ul>
        <li><a href="torchrec.modules.html#module-torchrec.modules.utils">module</a>
</li>
      </ul></li>
      <li>
    torchrec.optim

      <ul>
        <li><a href="torchrec.optim.html#module-0">module</a>, <a href="torchrec.optim.html#module-torchrec.optim">[1]</a>
</li>
      </ul></li>
      <li>
    torchrec.optim.clipping

      <ul>
        <li><a href="torchrec.optim.html#module-torchrec.optim.clipping">module</a>
</li>
      </ul></li>
      <li>
    torchrec.optim.fused

      <ul>
        <li><a href="torchrec.optim.html#module-torchrec.optim.fused">module</a>
</li>
      </ul></li>
      <li>
    torchrec.optim.keyed

      <ul>
        <li><a href="torchrec.optim.html#module-torchrec.optim.keyed">module</a>
</li>
      </ul></li>
      <li>
    torchrec.optim.warmup

      <ul>
        <li><a href="torchrec.optim.html#module-torchrec.optim.warmup">module</a>
</li>
      </ul></li>
      <li>
    torchrec.quant

      <ul>
        <li><a href="torchrec.quant.html#module-0">module</a>, <a href="torchrec.quant.html#module-torchrec.quant">[1]</a>
</li>
      </ul></li>
      <li>
    torchrec.quant.embedding_modules

      <ul>
        <li><a href="torchrec.quant.html#module-torchrec.quant.embedding_modules">module</a>
</li>
      </ul></li>
      <li>
    torchrec.sparse

      <ul>
        <li><a href="torchrec.sparse.html#module-0">module</a>, <a href="torchrec.sparse.html#module-torchrec.sparse">[1]</a>
</li>
      </ul></li>
      <li>
    torchrec.sparse.jagged_tensor

      <ul>
        <li><a href="torchrec.sparse.html#module-torchrec.sparse.jagged_tensor">module</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.torchrec_use_sync_collectives">torchrec_use_sync_collectives() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Perf.total">total (torchrec.distributed.planner.types.Perf property)</a>
</li>
      <li><a href="torchrec.distributed.html#id27">total_input_size (torchrec.distributed.comm_ops.ReduceScatterVInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.ReduceScatterVInfo.total_input_size">[1]</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.total_perf">total_perf (torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.ShardingOption.total_storage">total_storage (torchrec.distributed.planner.types.ShardingOption property)</a>
</li>
      <li><a href="torchrec.fx.html#torchrec.fx.tracer.Tracer.trace">trace() (torchrec.fx.tracer.Tracer method)</a>
</li>
      <li><a href="torchrec.fx.html#torchrec.fx.tracer.Tracer">Tracer (class in torchrec.fx.tracer)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.PipelineType.TRAIN_BASE">TRAIN_BASE (torchrec.distributed.types.PipelineType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.PipelineType.TRAIN_PREFETCH_SPARSE_DIST">TRAIN_PREFETCH_SPARSE_DIST (torchrec.distributed.types.PipelineType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.PipelineType.TRAIN_SPARSE_DIST">TRAIN_SPARSE_DIST (torchrec.distributed.types.PipelineType attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOne.training">training (torchrec.distributed.dist_data.EmbeddingsAllToOne attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOne.training">[1]</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce.training">(torchrec.distributed.dist_data.EmbeddingsAllToOneReduce attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.EmbeddingsAllToOneReduce.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTAllToAll.training">(torchrec.distributed.dist_data.KJTAllToAll attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAll.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.KJTOneToAll.training">(torchrec.distributed.dist_data.KJTOneToAll attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTOneToAll.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.MergePooledEmbeddingsModule.training">(torchrec.distributed.dist_data.MergePooledEmbeddingsModule attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.MergePooledEmbeddingsModule.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.training">(torchrec.distributed.dist_data.PooledEmbeddingsAllGather attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.training">(torchrec.distributed.dist_data.PooledEmbeddingsAllToAll attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.training">(torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.training">(torchrec.distributed.dist_data.SeqEmbeddingsAllToOne attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.training">(torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorAllToAll.training">(torchrec.distributed.dist_data.TensorAllToAll attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorAllToAll.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.TensorValuesAllToAll.training">(torchrec.distributed.dist_data.TensorValuesAllToAll attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.TensorValuesAllToAll.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll.training">(torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter.training">(torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter attribute)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter.training">[1]</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding.ShardedEmbeddingCollection.training">(torchrec.distributed.embedding.ShardedEmbeddingCollection attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.training">(torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.training">(torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferCPUGroupedEmbeddingsLookup.training">(torchrec.distributed.embedding_lookup.InferCPUGroupedEmbeddingsLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedEmbeddingsLookup.training">(torchrec.distributed.embedding_lookup.InferGroupedEmbeddingsLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.InferGroupedPooledEmbeddingsLookup.training">(torchrec.distributed.embedding_lookup.InferGroupedPooledEmbeddingsLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.training">(torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.training">(torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist.training">(torchrec.distributed.embedding_sharding.BaseEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist.training">(torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup.training">(torchrec.distributed.embedding_types.BaseEmbeddingLookup attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor.training">(torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.ShardedEmbeddingModule.training">(torchrec.distributed.embedding_types.ShardedEmbeddingModule attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.training">(torchrec.distributed.embeddingbag.ShardedEmbeddingBag attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection.training">(torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule.training">(torchrec.distributed.grouped_position_weighted.GroupedPositionWeightedModule attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embedding.ShardedManagedCollisionEmbeddingCollection.training">(torchrec.distributed.mc_embedding.ShardedManagedCollisionEmbeddingCollection attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_embeddingbag.ShardedManagedCollisionEmbeddingBagCollection.training">(torchrec.distributed.mc_embeddingbag.ShardedManagedCollisionEmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.mc_modules.ShardedManagedCollisionCollection.training">(torchrec.distributed.mc_modules.ShardedManagedCollisionCollection attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel.training">(torchrec.distributed.model_parallel.DistributedModelParallel attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEbcInputDist.training">(torchrec.distributed.quant_embeddingbag.ShardedQuantEbcInputDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection.training">(torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection.training">(torchrec.distributed.quant_embeddingbag.ShardedQuantFeatureProcessedEmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDist.training">(torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDistWithPermute.training">(torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDistWithPermute attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.training">(torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.training">(torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingDist.training">(torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.InferRwSparseFeaturesDist.training">(torchrec.distributed.sharding.rw_sharding.InferRwSparseFeaturesDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.training">(torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.training">(torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.training">(torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.training">(torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.training">(torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.training">(torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.training">(torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.training">(torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.types.ShardedModule.training">(torchrec.distributed.types.ShardedModule attribute)</a>
</li>
        <li><a href="torchrec.distributed.html#torchrec.distributed.utils.CopyableMixin.training">(torchrec.distributed.utils.CopyableMixin attribute)</a>
</li>
        <li><a href="torchrec.inference.html#torchrec.inference.modules.PredictModule.training">(torchrec.inference.modules.PredictModule attribute)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.DenseArch.training">(torchrec.models.deepfm.DenseArch attribute)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.FMInteractionArch.training">(torchrec.models.deepfm.FMInteractionArch attribute)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.OverArch.training">(torchrec.models.deepfm.OverArch attribute)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.SimpleDeepFMNN.training">(torchrec.models.deepfm.SimpleDeepFMNN attribute)</a>
</li>
        <li><a href="torchrec.models.html#torchrec.models.deepfm.SparseArch.training">(torchrec.models.deepfm.SparseArch attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.activation.SwishLayerNorm.training">(torchrec.modules.activation.SwishLayerNorm attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.CrossNet.training">(torchrec.modules.crossnet.CrossNet attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.LowRankCrossNet.training">(torchrec.modules.crossnet.LowRankCrossNet attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.LowRankMixtureCrossNet.training">(torchrec.modules.crossnet.LowRankMixtureCrossNet attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.crossnet.VectorCrossNet.training">(torchrec.modules.crossnet.VectorCrossNet attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.deepfm.DeepFM.training">(torchrec.modules.deepfm.DeepFM attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.deepfm.FactorizationMachine.training">(torchrec.modules.deepfm.FactorizationMachine attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection.training">(torchrec.modules.embedding_modules.EmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface.training">(torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection.training">(torchrec.modules.embedding_modules.EmbeddingCollection attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollectionInterface.training">(torchrec.modules.embedding_modules.EmbeddingCollectionInterface attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.BaseFeatureProcessor.training">(torchrec.modules.feature_processor.BaseFeatureProcessor attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.BaseGroupedFeatureProcessor.training">(torchrec.modules.feature_processor.BaseGroupedFeatureProcessor attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedModule.training">(torchrec.modules.feature_processor.PositionWeightedModule attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.feature_processor.PositionWeightedProcessor.training">(torchrec.modules.feature_processor.PositionWeightedProcessor attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.BaseManagedCollisionEmbeddingCollection.training">(torchrec.modules.mc_embedding_modules.BaseManagedCollisionEmbeddingCollection attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection.training">(torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection.training">(torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection.training">(torchrec.modules.mc_modules.ManagedCollisionCollection attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionModule.training">(torchrec.modules.mc_modules.ManagedCollisionModule attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHManagedCollisionModule.training">(torchrec.modules.mc_modules.MCHManagedCollisionModule attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mlp.MLP.training">(torchrec.modules.mlp.MLP attribute)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mlp.Perceptron.training">(torchrec.modules.mlp.Perceptron attribute)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection.training">(torchrec.quant.embedding_modules.EmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingCollection.training">(torchrec.quant.embedding_modules.EmbeddingCollection attribute)</a>
</li>
        <li><a href="torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection.training">(torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection attribute)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.ComputeJTDictToKJT.training">(torchrec.sparse.jagged_tensor.ComputeJTDictToKJT attribute)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.ComputeKJTToJTDict.training">(torchrec.sparse.jagged_tensor.ComputeKJTToJTDict attribute)</a>
</li>
      </ul></li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.trim_torch_package_prefix_from_typename">trim_torch_package_prefix_from_typename() (in module torchrec.inference.modules)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twcw_sharding.TwCwPooledEmbeddingSharding">TwCwPooledEmbeddingSharding (class in torchrec.distributed.sharding.twcw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist">TwPooledEmbeddingDist (class in torchrec.distributed.sharding.tw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding">TwPooledEmbeddingSharding (class in torchrec.distributed.sharding.tw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist">TwRwPooledEmbeddingDist (class in torchrec.distributed.sharding.twrw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding">TwRwPooledEmbeddingSharding (class in torchrec.distributed.sharding.twrw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist">TwRwSparseFeaturesDist (class in torchrec.distributed.sharding.twrw_sharding)</a>
</li>
      <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist">TwSparseFeaturesDist (class in torchrec.distributed.sharding.tw_sharding)</a>
</li>
      <li><a href="torchrec.inference.html#torchrec.inference.modules.BatchingMetadata.type">type (torchrec.inference.modules.BatchingMetadata attribute)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="U">U</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_types.InputDistOutputs.unbucketize_permute_tensor">unbucketize_permute_tensor (torchrec.distributed.embedding_types.InputDistOutputs attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.uncombined_embedding_dims">uncombined_embedding_dims() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.uncombined_embedding_dims">(torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding.uncombined_embedding_names">uncombined_embedding_names() (torchrec.distributed.embedding_sharding.EmbeddingSharding method)</a>

      <ul>
        <li><a href="torchrec.distributed.sharding.html#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.uncombined_embedding_names">(torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.uneven_sharding_perf_multiplier">uneven_sharding_perf_multiplier (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.unflatten_kjt_list">unflatten_kjt_list() (in module torchrec.sparse.jagged_tensor)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.PartitionByType.UNIFORM">UNIFORM (torchrec.distributed.planner.types.PartitionByType attribute)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.utils.LuusJaakolaSearch.uniform">uniform() (torchrec.distributed.planner.utils.LuusJaakolaSearch method)</a>
</li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.proposers.UniformProposer">UniformProposer (class in torchrec.distributed.planner.proposers)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.utils.SequenceVBEContext.unpadded_lengths">unpadded_lengths (torchrec.modules.utils.SequenceVBEContext attribute)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.unsync">unsync() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy.update_metadata_and_generate_eviction_scores">update_metadata_and_generate_eviction_scores() (torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy method)</a>

      <ul>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LFU_EvictionPolicy.update_metadata_and_generate_eviction_scores">(torchrec.modules.mc_modules.LFU_EvictionPolicy method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.LRU_EvictionPolicy.update_metadata_and_generate_eviction_scores">(torchrec.modules.mc_modules.LRU_EvictionPolicy method)</a>
</li>
        <li><a href="torchrec.modules.html#torchrec.modules.mc_modules.MCHEvictionPolicy.update_metadata_and_generate_eviction_scores">(torchrec.modules.mc_modules.MCHEvictionPolicy method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="V">V</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.optim.html#torchrec.optim.clipping.GradientClipping.VALUE">VALUE (torchrec.optim.clipping.GradientClipping attribute)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupStage.value">value (torchrec.optim.warmup.WarmupStage attribute)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.values">values() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.values">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor.values">(torchrec.sparse.jagged_tensor.KeyedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req">Variable_Batch_All2All_Pooled_Req (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.variable_batch_all2all_pooled_sync">variable_batch_all2all_pooled_sync() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait">Variable_Batch_All2All_Pooled_Wait (class in torchrec.distributed.comm_ops)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.variable_batch_alltoall_pooled">variable_batch_alltoall_pooled() (in module torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext.variable_batch_per_feature">variable_batch_per_feature (torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext attribute)</a>
</li>
      <li><a href="torchrec.distributed.html#id13">variable_batch_size (torchrec.distributed.comm_ops.All2AllSequenceInfo attribute)</a>, <a href="torchrec.distributed.html#torchrec.distributed.comm_ops.All2AllSequenceInfo.variable_batch_size">[1]</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.variable_stride_per_key">variable_stride_per_key() (torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.comm_ops.VariableBatchAll2AllPooledInfo">VariableBatchAll2AllPooledInfo (class in torchrec.distributed.comm_ops)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.embeddingbag.VariableBatchEmbeddingBagCollectionAwaitable">VariableBatchEmbeddingBagCollectionAwaitable (class in torchrec.distributed.embeddingbag)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll">VariableBatchPooledEmbeddingsAllToAll (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll">[1]</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter">VariableBatchPooledEmbeddingsReduceScatter (class in torchrec.distributed.dist_data)</a>, <a href="torchrec.distributed.sharding.html#torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter">[1]</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.crossnet.VectorCrossNet">VectorCrossNet (class in torchrec.modules.crossnet)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="W">W</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.html#torchrec.distributed.types.Awaitable.wait">wait() (torchrec.distributed.types.Awaitable method)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupOptimizer">WarmupOptimizer (class in torchrec.optim.warmup)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupPolicy">WarmupPolicy (class in torchrec.optim.warmup)</a>
</li>
      <li><a href="torchrec.optim.html#torchrec.optim.warmup.WarmupStage">WarmupStage (class in torchrec.optim.warmup)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.QuantConfig.weight">weight (torchrec.modules.embedding_configs.QuantConfig attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.weight_init_max">weight_init_max (torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
      <li><a href="torchrec.modules.html#torchrec.modules.embedding_configs.BaseEmbeddingConfig.weight_init_min">weight_init_min (torchrec.modules.embedding_configs.BaseEmbeddingConfig attribute)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.weighted_feature_bwd_compute_multiplier">weighted_feature_bwd_compute_multiplier (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.weights">weights() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.weights">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor.weights_or_none">weights_or_none() (torchrec.sparse.jagged_tensor.JaggedTensor method)</a>

      <ul>
        <li><a href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor.weights_or_none">(torchrec.sparse.jagged_tensor.KeyedJaggedTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchrec.distributed.planner.html#torchrec.distributed.planner.types.Topology.world_size">world_size (torchrec.distributed.planner.types.Topology property)</a>
</li>
      <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DataParallelWrapper.wrap">wrap() (torchrec.distributed.model_parallel.DataParallelWrapper method)</a>

      <ul>
        <li><a href="torchrec.distributed.html#torchrec.distributed.model_parallel.DefaultDataParallelWrapper.wrap">(torchrec.distributed.model_parallel.DefaultDataParallelWrapper method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="Z">Z</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchrec.optim.html#torchrec.optim.fused.EmptyFusedOptimizer.zero_grad">zero_grad() (torchrec.optim.fused.EmptyFusedOptimizer method)</a>

      <ul>
        <li><a href="torchrec.optim.html#torchrec.optim.fused.FusedOptimizer.zero_grad">(torchrec.optim.fused.FusedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.CombinedOptimizer.zero_grad">(torchrec.optim.keyed.CombinedOptimizer method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.KeyedOptimizerWrapper.zero_grad">(torchrec.optim.keyed.KeyedOptimizerWrapper method)</a>
</li>
        <li><a href="torchrec.optim.html#torchrec.optim.keyed.OptimizerWrapper.zero_grad">(torchrec.optim.keyed.OptimizerWrapper method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>



             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>